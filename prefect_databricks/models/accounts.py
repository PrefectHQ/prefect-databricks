# generated by datamodel-codegen:
#   filename:  account-2.0-aws.yaml
#   timestamp: 2022-06-29T19:47:26+00:00

from __future__ import annotations

from datetime import date, datetime
from enum import Enum
from typing import List, Literal, Optional
from uuid import UUID

from pydantic import BaseModel, Field, conint, constr


class AwsKeyInfo(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    key_alias: Optional[str] = Field(
        None, description='The AWS KMS key alias.', example='alias/projectKey1'
    )
    key_arn: str = Field(
        ...,
        description="The AWS KMS key's Amazon Resource Name (ARN).",
        example='arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321',
    )
    key_region: str = Field(
        ..., description='The AWS KMS key region.', example='us-east-1'
    )
    reuse_key_for_cluster_volumes: Optional[bool] = Field(
        None,
        description=(
            'This field applies only if the `use_cases` property includes `STORAGE`. If'
            ' this is set to `true` or omitted, the key is also used to encrypt cluster'
            ' EBS volumes. If you do not want to use this key for encrypting EBS'
            ' volumes, set to `false`..'
        ),
        example=True,
    )


class BudgetAlert(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    email_notifications: Optional[List[str]] = Field(
        None,
        description=(
            'List of email addresses to be notified when budget percentage is exceeded'
            ' in the given period'
        ),
    )
    min_percentage: Optional[conint(ge=1, le=100000)] = Field(
        None,
        description=(
            'Percentage of the target amount used in the currect period that will'
            ' trigger a notification'
        ),
    )


class BudgetFilter(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    __root__: str = Field(
        ...,
        description=(
            " SQL-like filter expression with workspaceId, SKU and tag. Usage in your"
            " account that matches this expression will be counted in this"
            " budget.\n\nSupported properties on left-hand side of comparison:\n *"
            " `workspaceId` - the ID of the workspace\n * `sku` - SKU of the cluster,"
            " e.g. `STANDARD_ALL_PURPOSE_COMPUTE` \n * `tag.tagName`, `tag.'tag name'`"
            " - tag of the cluster \n\nSupported comparison operators:\n * `=` - equal"
            " \n * `!=` - not equal \n\nSupported logical operators: `AND`,"
            " `OR`.\n\nExamples:\n * `workspaceId=123 OR"
            " (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')`\n *"
            " `workspaceId!=456`\n * `sku='STANDARD_ALL_PURPOSE_COMPUTE' OR"
            " sku='PREMIUM_ALL_PURPOSE_COMPUTE'`\n * `tag.name1='value1' AND"
            " tag.name2='value2'`\n "
        ),
        example=(
            "workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my"
            " tag'='my value')"
        ),
    )


class BudgetPeriod(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    __root__: str = Field(
        ...,
        description=(
            ' Period length in years, months, weeks and/or days.\n Examples: `1 month`,'
            ' `30 days`, `1 year, 2 months, 1 week, 2 days`\n '
        ),
        example='1 month',
    )


class StatusDailyItem(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    amount: Optional[str] = Field(
        None, description='Amount used in this day in USD', example='123.45'
    )
    date: Optional[date] = None


class BudgetWithStatus(BaseModel, extra=Extra.allow):
    """
    Budget configuration with daily status
    """

    class Config:
        allow_mutation = False

    alerts: Optional[List[BudgetAlert]] = None
    budget_id: Optional[UUID] = None
    creation_time: Optional[datetime] = None
    end_date: Optional[date] = Field(
        None, description='Optional end date of the budget'
    )
    filter: Optional[BudgetFilter] = None
    name: Optional[str] = Field(None, description='Human-readable name of the budget')
    period: Optional[BudgetPeriod] = None
    start_date: Optional[date] = Field(
        None, description='Start date of the budget period calculation'
    )
    status_daily: Optional[List[StatusDailyItem]] = Field(
        None, description='Amount used in the budget for each day (non-cumulative)'
    )
    target_amount: Optional[str] = Field(
        None,
        description='Target amount of the budget per period in USD',
        example='1234.56',
    )
    update_time: Optional[datetime] = None


class CreateAwsKeyInfo(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    key_alias: Optional[str] = Field(
        None, description='The AWS KMS key alias.', example='alias/projectKey1'
    )
    key_arn: str = Field(
        ...,
        description=(
            "The AWS KMS key's Amazon Resource Name (ARN). Note that the key's AWS"
            " region is inferred from the ARN."
        ),
        example='arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321',
    )
    reuse_key_for_cluster_volumes: Optional[bool] = Field(
        None,
        description=(
            'This field applies only if the `use_cases` property includes `STORAGE`. If'
            ' this is set to `true` or omitted, the key is also used to encrypt cluster'
            ' EBS volumes. To not use this key also for encrypting EBS volumes, set'
            ' this to `false`.'
        ),
        example=True,
    )


class StsRole(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    role_arn: Optional[str] = Field(
        None,
        description='The Amazon Resource Name (ARN) of the cross account role.',
        example='arn-aws-iam::111110000000:role/test_role',
    )


class AwsCredentials(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    sts_role: Optional[StsRole] = None


class CreateCredentialRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_credentials: AwsCredentials
    credentials_name: constr(min_length=4, max_length=256) = Field(
        ...,
        description='The human-readable name of the credential configuration object.',
        example='credential_1',
    )


class CreateCustomerManagedKeyRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_key_info: CreateAwsKeyInfo
    use_cases: List[str] = Field(
        ...,
        description=(
            "The cases that the key can be used for. Include one or both of these"
            " options:\n * `MANAGED_SERVICES`: Encrypts notebook and secret data in the"
            " control plane\n * `STORAGE`: Encrypts the workspace's root S3 bucket"
            " (root DBFS and system data) and optionally cluster EBS volumes."
        ),
        example=['MANAGED_SERVICES', 'STORAGE'],
    )


class RootBucketInfo(BaseModel, extra=Extra.allow):
    """
    Root S3 bucket information.
    """

    class Config:
        allow_mutation = False

    bucket_name: Optional[str] = Field(
        None, description='The name of the S3 bucket.', example='test-s3-bucket'
    )


class CreateStorageConfigurationRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    root_bucket_info: RootBucketInfo = Field(
        ..., description='Root S3 bucket information.'
    )
    storage_configuration_name: constr(min_length=4, max_length=256) = Field(
        ...,
        description='The human-readable name of the storage configuration.',
        example='storage_conf_1',
    )


class CreateVPCEndpointRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_vpc_endpoint_id: str = Field(
        ..., description='The ID of the VPC endpoint object in AWS.'
    )
    region: str = Field(
        ..., description='The AWS region in which this VPC endpoint object exists'
    )
    vpc_endpoint_name: constr(min_length=4, max_length=256) = Field(
        ..., description='The human-readable name of the storage configuration.'
    )


class CreateWorkspaceRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_region: str = Field(
        ...,
        description="The AWS region of the workspace's Data Plane.",
        example='us-west-2',
    )
    credentials_id: UUID = Field(
        ...,
        description="ID of the workspace's credential configuration object",
        example='ccc64f28-ebdc-4c89-add9-5dcb6d7727d8',
    )
    deployment_name: Optional[
        constr(regex=r'^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))|(EMPTY)$')
    ] = Field(
        None,
        description=(
            "The deployment name defines part of the subdomain for the workspace. The"
            " workspace URL for web application and REST APIs is"
            " `<workspace-deployment-name>.cloud.databricks.com`. For example, if the"
            " deployment name is `abcsales`, your workspace URL will be"
            " `https://abcsales.cloud.databricks.com`. Hyphens are allowed.  This"
            " property supports only the set of characters that are allowed in a"
            " subdomain.\n\nIf your account has a non-empty deployment name prefix at"
            " workspace creation time, the workspace deployment name changes so that"
            " the beginning has the account prefix and a hyphen. For example, if your"
            " account's deployment prefix is `acme` and the workspace deployment name"
            " is `workspace-1`, the `deployment_name` field becomes `acme-workspace-1`"
            " and that is the value that will be returned in JSON responses for the"
            " `deployment_name` field. The workspace URL is"
            " `acme-workspace-1.cloud.databricks.com`.\n\nIf your account has a"
            " non-empty deployment name prefix and you set `deployment_name` to the"
            " reserved keyword `EMPTY`, `deployment_name` is just the account prefix"
            " only. For example, if your account's deployment prefix is `acme` and the"
            " workspace deployment name is `EMPTY`, `deployment_name` becomes `acme`"
            " only and the workspace URL is `acme.cloud.databricks.com`.\n\nContact"
            " your Databricks representatives to add an account deployment name prefix"
            " to your account. If you do not have a deployment name prefix, the special"
            " deployment name value `EMPTY` is invalid.\n\nThis value must be unique"
            " across all non-deleted deployments across all AWS regions.\n\nIf a new"
            " workspace omits this property, the server generates a unique deployment"
            " name for you with the pattern `dbc-xxxxxxxx-xxxx`."
        ),
        example='workspace_1',
    )
    managed_services_customer_managed_key_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's managed services encryption key configuration"
            " object. This is used to encrypt the workspace's notebook and secret data"
            " in the control plane, as well as Databricks SQL queries and query"
            " history. The provided key configuration object property `use_cases` must"
            " contain `MANAGED_SERVICES`."
        ),
        example='849b3d6b-e68e-468d-b3e5-deb08b03c56d',
    )
    network_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's network configuration object. To use [AWS"
            " PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)"
            " (Public Preview), this field is required."
        ),
        example='fd0cc5bc-683c-47e9-b15e-144d7744a496',
    )
    pricing_tier: Optional[Literal['STANDARD', 'PREMIUM', 'ENTERPRISE']] = Field(
        None,
        description=(
            'The pricing tier of the workspace. If you do not provide this, the API'
            ' will default to the highest pricing tier available to your account.\nSee'
            ' https://databricks.com/product/aws-pricing for available pricing tier'
            ' information.'
        ),
        example='PREMIUM',
    )
    private_access_settings_id: Optional[UUID] = Field(
        None,
        description=(
            "Only used for PrivateLink, which is in Public Preview. This is the ID of"
            " the workspace's private access settings object. This ID must be specified"
            " for customers using [AWS"
            " PrivateLink](https://aws.amazon.com/privatelink/) for either front-end"
            " (user-to-workspace connection), back-end (data plane to control plane"
            " connection), or both connection types.\n\nBefore configuring PrivateLink,"
            " it is important to read the [Databricks article about"
            " PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)."
        ),
    )
    storage_configuration_id: UUID = Field(
        ...,
        description="The ID of the workspace's storage configuration object.",
        example='b43a6064-04c1-4e1c-88b6-d91e5b136b13',
    )
    storage_customer_managed_key_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's storage encryption key configuration object."
            " This is used to encrypt the workspace's root S3 bucket (root DBFS and"
            " system data) and optionally cluster EBS volumes. The provided key"
            " configuration object property `use_cases` must contain `STORAGE`."
        ),
    )
    workspace_name: str = Field(
        ...,
        description="The workspace's human-readable name.",
        example='My workspace 1',
    )


class StsRole1(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    external_id: Optional[str] = Field(
        None,
        description=(
            'The external ID that needs to be trusted by the cross-account role. This'
            ' is always your Databricks account ID.'
        ),
    )
    role_arn: Optional[str] = Field(
        None, description='The Amazon Resource Name (ARN) of the cross-account role.'
    )


class AwsCredentials1(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    sts_role: Optional[StsRole1] = None


class Credential(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None, description='The Databricks account ID that hosts the credential.'
    )
    aws_credentials: Optional[AwsCredentials1] = None
    creation_time: Optional[int] = Field(
        None, description='Time in epoch milliseconds when the credential was created.'
    )
    credentials_id: Optional[UUID] = Field(
        None, description='Databricks credential configuration ID.'
    )
    credentials_name: Optional[constr(min_length=4, max_length=256)] = Field(
        None,
        description='The human-readable name of the credential configuration object.',
    )


class CustomerManagedKey(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None,
        description='The Databricks account ID that holds the customer-managed key.',
    )
    aws_key_info: Optional[AwsKeyInfo] = None
    creation_time: Optional[int] = Field(
        None,
        description='Time in epoch milliseconds when the customer key was created.',
    )
    customer_managed_key_id: Optional[UUID] = Field(
        None, description='ID of the encryption key configuration object.'
    )
    use_cases: Optional[List[str]] = Field(
        None,
        description=(
            "The cases that the key can be used for. Include one or both of these"
            " options:\n * `MANAGED_SERVICES`: Encrypts notebook and secret data in the"
            " control plane\n * `STORAGE`: Encrypts the workspace's root S3 bucket"
            " (root DBFS and system data) and optionally cluster EBS volumes."
        ),
        example=['MANAGED_SERVICES', 'STORAGE'],
    )


class Error(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    message: Optional[str] = Field(None, description='Cause of the error')


class ListCredentialsResponse(BaseModel, extra=Extra.allow):
    """
    List of credential configuration objects.
    """

    class Config:
        allow_mutation = False

    __root__: List[Credential] = Field(
        ..., description='List of credential configuration objects.'
    )


class ListCustomerManagedKeysResponse(BaseModel, extra=Extra.allow):
    """
    Array of key configuration objects.
    """

    class Config:
        allow_mutation = False

    __root__: List[CustomerManagedKey] = Field(
        ..., description='Array of key configuration objects.'
    )


class LogDeliveryConfigStatus(Enum):
    """
    Status of log delivery configuration. Set to `ENABLED` (enabled) or `DISABLED` (disabled). Defaults to `ENABLED`. You can [enable or disable the configuration](#operation/patch-log-delivery-config-status) later. Deletion of a configuration is not supported, so disable a log delivery configuration that is no longer needed.
    """

    enabled = 'ENABLED'
    disabled = 'DISABLED'


class LogDeliveryStatus(BaseModel, extra=Extra.allow):
    """
    Databricks log delivery status.
    """

    class Config:
        allow_mutation = False

    last_attempt_time: Optional[datetime] = Field(
        None, description='The UTC time for the latest log delivery attempt.'
    )
    last_successful_attempt_time: Optional[datetime] = Field(
        None, description='The UTC time for the latest successful log delivery.'
    )
    message: Optional[str] = Field(
        None,
        description=(
            'Informative message about the latest log delivery attempt. If the log'
            ' delivery fails with USER_FAILURE, error details will be provided for'
            ' fixing misconfigurations in cloud permissions.'
        ),
    )
    status: Optional[
        Literal['CREATED', 'SUCCEEDED', 'USER_FAILURE', 'SYSTEM_FAILURE', 'NOT_FOUND']
    ] = Field(
        None,
        description=(
            "The status string for log delivery. Possible values are: * `CREATED`:"
            " There were no log delivery attempts since the config was created. *"
            " `SUCCEEDED`: The latest attempt of log delivery has succeeded completely."
            " * `USER_FAILURE`: The latest attempt of log delivery failed because of"
            " misconfiguration of customer provided permissions on role or storage. *"
            " `SYSTEM_FAILURE`: The latest attempt of log delivery failed because of an"
            " Databricks internal error. Contact support if it doesn't go away soon. *"
            " `NOT_FOUND`: The log delivery status as the configuration has been"
            " disabled since the release of this feature or there are no workspaces in"
            " the account."
        ),
    )


class NetworkHealth(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    error_message: Optional[str] = Field(None, description='Details of the error.')
    error_type: Optional[
        Literal['credentials', 'vpc', 'subnet', 'securityGroup', 'networkAcl']
    ] = Field(
        None,
        description=(
            'The AWS resource associated with this error: credentials, VPC, subnet,'
            ' security group, or network ACL.'
        ),
    )


class NetworkVpcEndpoints(BaseModel, extra=Extra.allow):
    """
    If specified, contains the VPC endpoints used to allow cluster communication from this VPC over [AWS PrivateLink](https://aws.amazon.com/privatelink/).
    """

    class Config:
        allow_mutation = False

    dataplane_relay: List[UUID] = Field(
        ...,
        description=(
            'The VPC endpoint ID used by this Network to access the Databricks secure'
            ' cluster connectivity relay. See [Secure Cluster'
            ' Connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html).\n\nThis'
            ' is a list type for future compatibility, but currently only one VPC'
            ' endpoint ID should be supplied.\n\nNote: This is the Databricks-specific'
            ' ID of the VPC endpoint object in the Account API, not the AWS VPC'
            ' endpoint ID that you see for your endpoint in the AWS Console.'
        ),
    )
    rest_api: List[UUID] = Field(
        ...,
        description=(
            'The VPC endpoint ID used by this Network to access the Databricks REST'
            ' API. Databricks clusters make calls to our REST API as part of cluster'
            ' creation, mlflow tracking, and many other features. Thus, this is'
            ' required even if your workspace allows public access to the REST'
            ' API.\n\nThis is a list type for future compatibility, but currently only'
            ' one VPC endpoint ID should be supplied.\n\nNote: This is the'
            ' Databricks-specific ID of the VPC endpoint object in the Account API, not'
            ' the AWS VPC endpoint ID that you see for your endpoint in the AWS'
            ' Console.'
        ),
    )


class NetworkWarning(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    warning_message: Optional[str] = Field(None, description='Details of the warning.')
    warning_type: Optional[Literal['subnet', 'securityGroup']] = Field(
        None,
        description=(
            'The AWS resource associated with this warning: a subnet or a security'
            ' group.'
        ),
    )


class PrivateAccessSettings(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None, description='The Databricks account ID that hosts the credential.'
    )
    allowed_vpc_endpoint_ids: Optional[List[UUID]] = Field(
        None,
        description=(
            'An array of Databricks VPC endpoint IDs. This is the Databricks ID'
            ' returned when registering the VPC endpoint configuration in your'
            ' Databricks account. This is _not_ the ID of the VPC endpoint in'
            ' AWS.\n\nOnly used when `private_access_level` is set to `ENDPOINT`. This'
            ' is an allow list of VPC endpoints registered in your Databricks account'
            ' that can connect to your workspace over AWS PrivateLink.\n\nNote: if'
            ' hybrid access to your workspace is enabled by setting'
            ' `public_access_enabled` to `true`, then this control only works for'
            ' PrivateLink connections. To control how your workspace is accessed via'
            ' public internet, see the article on [IP access'
            ' lists](https://docs.databricks.com/security/network/ip-access-list.html).'
        ),
    )
    private_access_level: Optional[Literal['ANY', 'ACCOUNT', 'ENDPOINT']] = Field(
        None,
        description=(
            'The private access level controls which VPC endpoints can connect to the'
            ' UI or API of any workspace that attaches this private access settings'
            ' object.\n* `ANY` (the default): Any VPC endpoint can connect to your'
            ' workspace.\n* `ACCOUNT` level access lets only VPC endpoints that are'
            ' registered in your Databricks account connect to your workspace.\n*'
            ' `ENDPOINT` level access lets only specified VPC endpoints connect to your'
            ' workspace. See the `allowed_vpc_endpoint_ids` for details.'
        ),
        example='ENDPOINT',
    )
    private_access_settings_id: Optional[UUID] = Field(
        None, description='Databricks private access settings ID.'
    )
    private_access_settings_name: Optional[
        constr(min_length=4, max_length=256)
    ] = Field(
        None,
        description='The human-readable name of the private access settings object.',
    )
    public_access_enabled: Optional[bool] = Field(
        None,
        description=(
            'Determines if the workspace can be accessed over public internet. For'
            ' fully private workspaces, you can optionally specify `false`, but only if'
            ' you implement both the front-end and the back-end PrivateLink'
            ' connections. Otherwise, specify `true`, which means that public access is'
            ' still enabled.'
        ),
    )
    region: Optional[str] = Field(
        None,
        description=(
            'The AWS region for workspaces attached to this private access settings'
            ' object.'
        ),
    )


class RootBucketInfo1(BaseModel, extra=Extra.allow):
    """
    Root S3 bucket information.
    """

    class Config:
        allow_mutation = False

    bucket_name: Optional[str] = Field(None, description='The name of the S3 bucket.')


class StorageConfiguration(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None, description='The Databricks account ID that hosts the credential.'
    )
    creation_time: Optional[int] = Field(
        None,
        description=(
            'Time in epoch milliseconds when the storage configuration was created.'
        ),
    )
    root_bucket_info: Optional[RootBucketInfo1] = Field(
        None, description='Root S3 bucket information.'
    )
    storage_configuration_id: Optional[UUID] = Field(
        None, description='Databricks storage configuration ID.'
    )
    storage_configuration_name: Optional[constr(min_length=4, max_length=256)] = Field(
        None, description='The human-readable name of the storage configuration.'
    )


class UpdateLogDeliveryConfigurationStatusRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    status: LogDeliveryConfigStatus


class UpdateWorkspaceRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_region: Optional[str] = Field(
        None,
        description=(
            "The AWS region of the workspace's Data Plane. For example, `us-west-2`."
            " This parameter is available only for updating failed workspaces."
        ),
        example='us-west-2',
    )
    credentials_id: Optional[UUID] = Field(
        None,
        description=(
            "ID of the workspace's credential configuration object. This parameter is"
            " available for updating both failed and running workspaces."
        ),
    )
    managed_services_customer_managed_key_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's managed services encryption key configuration"
            " object. This parameter is available only for updating failed workspaces."
        ),
    )
    network_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's network configuration object. Used only if you"
            " already use a customer-managed VPC. This change is supported only if you"
            " specified a network configuration ID when the workspace was created. In"
            " other words, you cannot switch from a Databricks-managed VPC to a"
            " customer-managed VPC. This parameter is available for updating both"
            " failed and running workspaces. Note: You cannot use a network"
            " configuration update in this API to add support for PrivateLink (in"
            " Public Preview). To add PrivateLink to an existing workspace, contact"
            " your Databricks representative."
        ),
    )
    storage_configuration_id: Optional[UUID] = Field(
        None,
        description=(
            "The ID of the workspace's storage configuration object. This parameter is"
            " available only for updating failed workspaces."
        ),
    )
    storage_customer_managed_key_id: Optional[UUID] = Field(
        None,
        description=(
            'The ID of the key configuration object for workspace storage. This'
            ' parameter is available for updating both failed and running workspaces.'
        ),
    )


class UpsertPrivateAccessSettingsRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    allowed_vpc_endpoint_ids: Optional[List[UUID]] = Field(
        None,
        description=(
            'An array of Databricks VPC endpoint IDs. This is the Databricks ID that is'
            ' returned when registering the VPC endpoint configuration in your'
            ' Databricks account. This is not the ID of the VPC endpoint in'
            ' AWS.\n\nOnly used when `private_access_level` is set to `ENDPOINT`. This'
            ' is an allow list of VPC endpoints that in your account that can connect'
            ' to your workspace over AWS PrivateLink.\n\nIf hybrid access to your'
            ' workspace is enabled by setting `public_access_enabled` to `true`, then'
            ' this control only works for PrivateLink connections. To control how your'
            ' workspace is accessed via public internet, see the article for [IP access'
            ' lists](https://docs.databricks.com/security/network/ip-access-list.html).'
        ),
    )
    private_access_level: Optional[Literal['ANY', 'ACCOUNT', 'ENDPOINT']] = Field(
        'ANY',
        description=(
            'The private access level controls which VPC endpoints can connect to the'
            ' UI or API of any workspace that attaches this private access settings'
            ' object.\n* `ANY` level access lets any VPC endpoint connect to your'
            ' workspace.\n* `ACCOUNT` level access lets only VPC endpoints that are'
            ' registered in your Databricks account connect to your workspace.\n*'
            ' `ENDPOINT` level access lets only specified VPC endpoints connect to your'
            ' workspace. Please see the `allowed_vpc_endpoint_ids` documentation for'
            ' more details.'
        ),
        example='ENDPOINT',
    )
    private_access_settings_name: constr(min_length=4, max_length=256) = Field(
        ...,
        description='The human-readable name of the private access settings object.',
    )
    public_access_enabled: Optional[bool] = Field(
        False,
        description=(
            'Determines if the workspace can be accessed over public internet. For'
            ' fully private workspaces, you can optionally specify `false`, but only if'
            ' you implement both the front-end and the back-end PrivateLink'
            ' connections. Otherwise, specify `true`, which means that public access is'
            ' still enabled.'
        ),
    )
    region: str = Field(
        ...,
        description=(
            'The AWS region for workspaces associated with this private access settings'
            ' object. This must be a [region that Databricks supports for'
            ' PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).'
        ),
    )


class UsageDownloadMonth(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    __root__: constr(regex=r'2[0-9][0-9][0-9]-(0[1-9]|1[012])') = Field(
        ...,
        description=(
            'Format specification for month in the format `YYYY-MM`. This is used to'
            ' specify billable usage `start_month` and `end_month` properties. Note'
            ' that billable usage logs are unavailable before March 2019 (`2019-03`).'
        ),
    )


class VPCEndpoint(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None,
        description=(
            'The Databricks account ID that hosts the VPC endpoint configuration.'
        ),
    )
    aws_account_id: Optional[str] = Field(
        None, description='The AWS Account in which the VPC endpoint object exists.'
    )
    aws_endpoint_service_id: Optional[str] = Field(
        None,
        description=(
            'The ID of the Databricks [endpoint'
            ' service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html)'
            ' that this VPC endpoint is connected to. Please find the list of endpoint'
            ' service IDs for each supported region in the [Databricks PrivateLink'
            ' documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).'
        ),
    )
    aws_vpc_endpoint_id: Optional[str] = Field(
        None, description='The ID of the VPC endpoint object in AWS.'
    )
    region: Optional[str] = Field(
        None, description='The AWS region in which this VPC endpoint object exists.'
    )
    state: Optional[str] = Field(
        None,
        description=(
            'The current state (such as `available` or `rejected`) of the VPC endpoint.'
            ' Derived from AWS. For the full set of values, see [AWS'
            ' DescribeVpcEndpoint'
            ' documentation](https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-vpc-endpoints.html)'
            ' for details.'
        ),
    )
    use_case: Optional[Literal['WORKSPACE_ACCESS', 'DATAPLANE_RELAY_ACCESS']] = Field(
        None,
        description=(
            'This enumeration represents the type of Databricks VPC [endpoint'
            ' service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html)'
            ' that was used when creating this VPC endpoint.\n\nIf the VPC endpoint'
            ' connects to the Databricks control plane for either the front-end'
            ' connection or the back-end REST API connection, the value is'
            ' `WORKSPACE_ACCESS`.\n\nIf the VPC endpoint connects to the Databricks'
            ' workspace for the back-end [secure cluster'
            ' connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html)'
            ' relay, the value is `DATAPLANE_RELAY_ACCESS`.'
        ),
    )
    vpc_endpoint_id: Optional[UUID] = Field(
        None,
        description=(
            'Databricks VPC endpoint ID. This is the Databricks-specific name of the'
            ' VPC endpoint. Do not confuse this with the `aws_vpc_endpoint_id`, which'
            ' is the ID within AWS of the VPC endpoint.'
        ),
    )
    vpc_endpoint_name: Optional[constr(min_length=4, max_length=256)] = Field(
        None, description='The human-readable name of the storage configuration.'
    )


class Workspace(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(None, description='Databricks account ID')
    aws_region: Optional[str] = Field(
        None,
        description=(
            'The AWS region of the workspace Data Plane. For example, `us-west-2`.'
        ),
    )
    creation_time: Optional[int] = Field(
        None, description='Time in epoch milliseconds when the workspace was created.'
    )
    credentials_id: Optional[UUID] = Field(
        None, description="ID of the workspace's credential configuration object."
    )
    deployment_name: Optional[constr(max_length=64)] = Field(
        None,
        description=(
            'The deployment name defines part of the subdomain for the workspace. The'
            ' workspace URL for web application and REST APIs is'
            ' `<deployment-name>.cloud.databricks.com`.\n\nThis value must be unique'
            ' across all non-deleted deployments across all AWS regions.'
        ),
    )
    managed_services_customer_managed_key_id: Optional[UUID] = Field(
        None, description='ID of the key configuration for encrypting managed services.'
    )
    pricing_tier: Optional[
        Literal[
            'UNKNOWN',
            'COMMUNITY_EDITION',
            'STANDARD',
            'PREMIUM',
            'ENTERPRISE',
            'DEDICATED',
        ]
    ] = Field(
        None,
        description=(
            'The pricing tier of the workspace.\nSee'
            ' https://databricks.com/product/aws-pricing for available pricing tier'
            ' information.'
        ),
        example='PREMIUM',
    )
    private_access_settings_id: Optional[UUID] = Field(
        None,
        description=(
            "Only used for PrivateLink, which is in Public Preview. This is the ID of"
            " the workspace's private access settings object. This ID must be specified"
            " for customers using [AWS"
            " PrivateLink](https://aws.amazon.com/privatelink/) for either front-end"
            " (user-to-workspace connection), back-end (data plane to control plane"
            " connection), or both connection types.\n\nBefore configuring PrivateLink,"
            " it is important to read the [Databricks article about"
            " PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)."
        ),
    )
    storage_configuration_id: Optional[UUID] = Field(
        None, description="ID of the workspace's storage configuration object."
    )
    storage_customer_managed_key_id: Optional[UUID] = Field(
        None,
        description='ID of the key configuration for encrypting workspace storage.',
    )
    workspace_id: Optional[int] = Field(None, description='Workspace ID.')
    workspace_name: Optional[constr(min_length=1, max_length=100)] = Field(
        None, description='The human-readable name of the workspace.'
    )
    workspace_status: Optional[
        Literal[
            'NOT_PROVISIONED',
            'PROVISIONING',
            'RUNNING',
            'FAILED',
            'BANNED',
            'CANCELLING',
        ]
    ] = Field(
        None,
        description=(
            'The status of the workspace.\nFor workspace creation, it is typically'
            ' initially `PROVISIONING`. Continue to check the status until the status'
            ' is `RUNNING`. For detailed instructions of creating a new workspace with'
            ' this API **including error handling** see [Create a new workspace using'
            ' the Account Management'
            ' API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).'
        ),
        example='RUNNING',
    )
    workspace_status_message: Optional[str] = Field(
        None,
        description='Message describing the current workspace status.',
        example='Workspace resources are being set up.',
    )


class WorkspaceEncryptionKeyRecord(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    aws_key_info: Optional[AwsKeyInfo] = None
    customer_managed_key_id: Optional[UUID] = Field(
        None, description='ID of the encryption key configuration object.'
    )
    key_status: Optional[Literal['UNKNOWN', 'ATTACHED', 'DETACHED']] = Field(
        None, example='ATTACHED'
    )
    record_id: Optional[UUID] = Field(
        None, description='ID for the workspace-key mapping record'
    )
    update_time: Optional[int] = Field(
        None, description='Time in epoch milliseconds when the record was added.'
    )
    use_case: Optional[Literal['MANAGED_SERVICES', 'STORAGE']] = Field(
        None,
        description=(
            "Possible values are:\n - `MANAGED_SERVICES`: Encrypts notebook and secret"
            " data in the control plane\n - `STORAGE`: Encrypts the workspace's root S3"
            " bucket (root DBFS and system data) and optionally cluster EBS volumes."
        ),
        example='STORAGE',
    )
    workspace_id: Optional[int] = Field(None, description='Workspace ID.')


class WorkspaceId(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    __root__: int


class BudgetCreateRequest(BaseModel, extra=Extra.allow):
    """
    Budget configuration to be created
    """

    class Config:
        allow_mutation = False

    alerts: Optional[List[BudgetAlert]] = None
    end_date: Optional[date] = Field(
        None, description='Optional end date of the budget'
    )
    filter: BudgetFilter
    name: str = Field(..., description='Human-readable name of the budget')
    period: BudgetPeriod
    start_date: date = Field(
        ..., description='Start date of the budget period calculation'
    )
    target_amount: str = Field(
        ...,
        description='Target amount of the budget per period in USD',
        example='1234.56',
    )


class BudgetList(BaseModel, extra=Extra.allow):
    """
    List of Budgets
    """

    class Config:
        allow_mutation = False

    budgets: Optional[List[BudgetWithStatus]] = None


class CreateLogDeliveryConfigurationParams(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    config_name: Optional[constr(min_length=0, max_length=255)] = Field(
        None,
        description=(
            'The optional human-readable name of the log delivery configuration.'
            ' Defaults to empty.'
        ),
    )
    credentials_id: Optional[UUID] = Field(
        None,
        description=(
            'The ID for a [Databricks credential'
            ' configuration](#operation/create-credential-config) that represents the'
            ' AWS IAM role with policy and trust relationship as described in the main'
            ' billable usage documentation page. See [Configure billable usage'
            ' delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).'
        ),
    )
    delivery_path_prefix: Optional[str] = Field(
        None,
        description=(
            'The optional delivery path prefix within AWS S3 storage. Defaults to'
            ' empty, which means that logs are delivered to the root of the bucket.'
            ' This must be a valid S3 object key. This must not start or end with a'
            ' slash character.'
        ),
    )
    delivery_start_time: Optional[
        constr(regex=r'2[0-9][0-9][0-9]-(0[1-9]|1[012])')
    ] = Field(
        None,
        description=(
            'This field applies only if `log_type` is `BILLABLE_USAGE`. This is the'
            ' optional start month and year for delivery, specified in `YYYY-MM`'
            ' format. Defaults to current year and month.  `BILLABLE_USAGE` logs are'
            ' not available for usage before March 2019 (`2019-03`).'
        ),
    )
    log_type: Optional[Literal['BILLABLE_USAGE', 'AUDIT_LOGS']] = Field(
        None,
        description=(
            'Log delivery type. Supported values are:\n\n* `BILLABLE_USAGE` — Configure'
            ' [billable usage log'
            ' delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).'
            ' For the CSV schema, see the [View billable'
            ' usage](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n\n*'
            ' `AUDIT_LOGS` — Configure [audit log'
            ' delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).'
            ' For the JSON schema, see [Configure audit'
            ' logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)'
        ),
    )
    output_format: Optional[Literal['CSV', 'JSON']] = Field(
        None,
        description=(
            'The file type of log delivery.\n\n*  If `log_type` is `BILLABLE_USAGE`,'
            ' this value must be `CSV`. Only the CSV (comma-separated values) format is'
            ' supported. For the schema, see the [View billable'
            ' usage](https://docs.databricks.com/administration-guide/account-settings/usage.html)\n*'
            ' If `log_type` is `AUDIT_LOGS`, this value must be `JSON`. Only the JSON'
            ' (JavaScript Object Notation) format is supported. For the schema, see the'
            ' [Configuring audit'
            ' logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).'
        ),
    )
    status: Optional[LogDeliveryConfigStatus] = None
    storage_configuration_id: Optional[UUID] = Field(
        None,
        description=(
            '"The ID for a [Databricks storage'
            ' configuration](#operation/create-storage-config)  that represents the S3'
            ' bucket with bucket policy as described in the main billable usage'
            ' documentation page. See [Configure billable usage'
            ' delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html)."'
        ),
    )
    workspace_ids_filter: Optional[List[WorkspaceId]] = Field(
        None,
        description=(
            "Optional filter of workspace IDs to deliver logs for. By default the"
            " workspace filter is empty and log delivery applies at the account level,"
            " delivering workspace level logs for all workspaces in your account, plus"
            " account level logs. You can optionally set this field to an array of"
            " workspace IDs (each one is an `int64`) to which log delivery should apply"
            " to, in which case only workspace level logs relating to the specified"
            " workspaces will be delivered.\nIf you plan to use different log delivery"
            " configurations for different workspaces, set this field explicitly. Be"
            " aware that delivery configurations mentioning specific workspaces won't"
            " apply to new workspaces created in the future, and delivery won't include"
            " account level logs.\nFor some types of Databricks deployments there is"
            " only one workspace per account ID, so this field is unnecessary."
        ),
    )


class CreateNetworkRequest(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    network_name: constr(min_length=4, max_length=256) = Field(
        ..., description='The human-readable name of the network configuration.'
    )
    security_group_ids: List[str] = Field(
        ...,
        description=(
            'IDs of 1 to 5 security groups associated with this network. Security'
            ' groups IDs **cannot** be used in multiple network configurations.'
        ),
        max_length=5,
        min_length=1,
    )
    subnet_ids: List[str] = Field(
        ...,
        description=(
            'IDs of at least 2 subnets associated with this network. Subnet IDs'
            ' **cannot** be used in multiple network configurations.'
        ),
        min_length=2,
    )
    vpc_endpoints: Optional[NetworkVpcEndpoints] = None
    vpc_id: str = Field(
        ...,
        description=(
            'The ID of the VPC associated with this network. VPC IDs can be used in'
            ' multiple network configurations.'
        ),
    )


class ListPrivateAccessSettingsResponse(BaseModel, extra=Extra.allow):
    """
    Private access settings objects.
    """

    class Config:
        allow_mutation = False

    __root__: List[PrivateAccessSettings] = Field(
        ..., description='Private access settings objects.'
    )


class ListStorageConfigurationsResponse(BaseModel, extra=Extra.allow):
    """
    Storage configuration objects.
    """

    class Config:
        allow_mutation = False

    __root__: List[StorageConfiguration] = Field(
        ..., description='Storage configuration objects.'
    )


class ListVPCEndpointsResponse(BaseModel, extra=Extra.allow):
    """
    List VPC endpoint configurations.
    """

    class Config:
        allow_mutation = False

    __root__: List[VPCEndpoint] = Field(
        ..., description='List VPC endpoint configurations.'
    )


class ListWorkspaceEncryptionKeyRecordsResponse(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    workspace_encryption_key_records: Optional[
        List[WorkspaceEncryptionKeyRecord]
    ] = Field(None, alias='workspaceEncryptionKeyRecords')


class ListWorkspacesResponse(BaseModel, extra=Extra.allow):
    """
    An array of workspaces.
    """

    class Config:
        allow_mutation = False

    __root__: List[Workspace] = Field(..., description='An array of workspaces.')


class LogDeliveryConfiguration(CreateLogDeliveryConfigurationParams):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None,
        description=(
            'The Databricks account ID that hosts the log delivery configuration.'
        ),
    )
    config_id: Optional[UUID] = Field(
        None, description='Databricks log delivery configuration ID.'
    )
    creation_time: Optional[int] = Field(
        None,
        description=(
            'Time in epoch milliseconds when the log delivery configuration was'
            ' created.'
        ),
    )
    log_delivery_status: Optional[LogDeliveryStatus] = Field(
        None, description='Databricks log delivery status.'
    )
    update_time: Optional[int] = Field(
        None,
        description=(
            'Time in epoch milliseconds when the log delivery configuration was'
            ' updated.'
        ),
    )


class Network(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    account_id: Optional[UUID] = Field(
        None,
        description=(
            'The Databricks account ID associated with this network configuration.'
        ),
    )
    creation_time: Optional[int] = Field(
        None, description='Time in epoch milliseconds when the network was created.'
    )
    error_messages: Optional[List[NetworkHealth]] = Field(
        None, description='Array of error messages about the network configuration.'
    )
    network_id: Optional[UUID] = Field(
        None, description='The Databricks network configuration ID.'
    )
    network_name: Optional[constr(min_length=4, max_length=256)] = Field(
        None, description='The human-readable name of the network configuration.'
    )
    security_group_ids: Optional[List[str]] = Field(None, max_length=5, min_length=1)
    subnet_ids: Optional[List[str]] = Field(None, min_length=2)
    vpc_endpoints: Optional[NetworkVpcEndpoints] = None
    vpc_id: Optional[str] = Field(
        None,
        description=(
            'The ID of the VPC associated with this network configuration. VPC IDs can'
            ' be used in multiple networks.'
        ),
    )
    vpc_status: Optional[Literal['UNATTACHED', 'VALID', 'BROKEN', 'WARNED']] = Field(
        None,
        description=(
            'The status of this network configuration object in terms of its use in a'
            ' workspace:\n\n* `UNATTACHED`: Unattached.\n\n * `VALID`: Valid.\n\n *'
            ' `BROKEN`: Broken.\n\n * `WARNED`: Warned.'
        ),
    )
    warning_messages: Optional[List[NetworkWarning]] = Field(
        None, description='Array of warning messages about the network configuration.'
    )
    workspace_id: Optional[int] = Field(
        None, description='Workspace ID associated with this network configuration.'
    )


class WrappedCreateLogDeliveryConfiguration(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    log_delivery_configuration: Optional[CreateLogDeliveryConfigurationParams] = None


class WrappedLogDeliveryConfiguration(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    log_delivery_configuration: Optional[LogDeliveryConfiguration] = None


class WrappedLogDeliveryConfigurations(BaseModel, extra=Extra.allow):
    class Config:
        allow_mutation = False

    log_delivery_configurations: Optional[List[LogDeliveryConfiguration]] = None


class ListNetworksResponse(BaseModel, extra=Extra.allow):
    """
    Array of network configuration objects.
    """

    class Config:
        allow_mutation = False

    __root__: List[Network] = Field(
        ..., description='Array of network configuration objects.'
    )
