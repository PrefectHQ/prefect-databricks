components:
  parameters:
    account_id:
      description: Databricks account ID of any type. For non-E2 account types, get
        your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html).
      in: path
      name: account_id
      required: true
      schema:
        format: uuid
        type: string
    account_id_dual_use:
      description: Databricks account ID. When you create or manage workspaces, your
        account must be on the E2 version of the platform or on a select custom plan
        that allows multiple workspaces per account. If you are configuring log delivery,
        all account types are supported. For non-E2 account types, get your account
        ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html).
      in: path
      name: account_id
      required: true
      schema:
        format: uuid
        type: string
    account_id_e2:
      description: Databricks account ID. Your account must be on the E2 version of
        the platform or on a select custom plan that allows multiple workspaces per
        account.
      in: path
      name: account_id
      required: true
      schema:
        format: uuid
        type: string
  responses:
    BadRequest:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request is malformed.
    Conflict:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request conflicts with the current state of the target resource.
    Forbidden:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request is forbidden from being fulfilled.
    InternalError:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request is not handled correctly due to a server error.
    NotFound:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The requested resource does not exist.
    ServiceUnavailable:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The service is unavailable.
    Unauthenticated:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
      description: The request is unauthenticated. The user's credentials are missing
        or incorrect.
  schemas:
    AwsKeyInfo:
      properties:
        key_alias:
          description: The AWS KMS key alias.
          example: alias/projectKey1
          type: string
        key_arn:
          description: The AWS KMS key's Amazon Resource Name (ARN).
          example: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
          type: string
        key_region:
          description: The AWS KMS key region.
          example: us-east-1
          type: string
        reuse_key_for_cluster_volumes:
          description: This field applies only if the `use_cases` property includes
            `STORAGE`. If this is set to `true` or omitted, the key is also used to
            encrypt cluster EBS volumes. If you do not want to use this key for encrypting
            EBS volumes, set to `false`..
          example: true
          type: boolean
      required:
      - key_arn
      - key_region
      type: object
    BudgetAlert:
      properties:
        email_notifications:
          description: List of email addresses to be notified when budget percentage
            is exceeded in the given period
          items:
            example: foo@bar.com
            type: string
          type: array
        min_percentage:
          description: Percentage of the target amount used in the currect period
            that will trigger a notification
          maximum: 100000
          minimum: 1
          type: integer
      type: object
    BudgetCreateRequest:
      description: Budget configuration to be created
      properties:
        alerts:
          items:
            $ref: '#/components/schemas/BudgetAlert'
          type: array
        end_date:
          description: Optional end date of the budget
          format: date
          type: string
        filter:
          $ref: '#/components/schemas/BudgetFilter'
        name:
          description: Human-readable name of the budget
          type: string
        period:
          $ref: '#/components/schemas/BudgetPeriod'
        start_date:
          description: Start date of the budget period calculation
          format: date
          type: string
        target_amount:
          description: Target amount of the budget per period in USD
          example: '1234.56'
          type: string
      required:
      - name
      - period
      - start_date
      - target_amount
      - filter
      type: object
    BudgetFilter:
      description: " SQL-like filter expression with workspaceId, SKU and tag. Usage\
        \ in your account that matches this expression will be counted in this budget.\n\
        \nSupported properties on left-hand side of comparison:\n * `workspaceId`\
        \ - the ID of the workspace\n * `sku` - SKU of the cluster, e.g. `STANDARD_ALL_PURPOSE_COMPUTE`\
        \ \n * `tag.tagName`, `tag.'tag name'` - tag of the cluster \n\nSupported\
        \ comparison operators:\n * `=` - equal \n * `!=` - not equal \n\nSupported\
        \ logical operators: `AND`, `OR`.\n\nExamples:\n * `workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE'\
        \ AND tag.'my tag'='my value')`\n * `workspaceId!=456`\n * `sku='STANDARD_ALL_PURPOSE_COMPUTE'\
        \ OR sku='PREMIUM_ALL_PURPOSE_COMPUTE'`\n * `tag.name1='value1' AND tag.name2='value2'`\n\
        \ "
      example: workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my
        tag'='my value')
      type: string
    BudgetList:
      description: List of Budgets
      properties:
        budgets:
          items:
            $ref: '#/components/schemas/BudgetWithStatus'
          type: array
      type: object
    BudgetPeriod:
      description: " Period length in years, months, weeks and/or days.\n Examples:\
        \ `1 month`, `30 days`, `1 year, 2 months, 1 week, 2 days`\n "
      example: 1 month
      type: string
    BudgetWithStatus:
      description: Budget configuration with daily status
      properties:
        alerts:
          items:
            $ref: '#/components/schemas/BudgetAlert'
          type: array
        budget_id:
          format: uuid
          type: string
        creation_time:
          format: date-time
          type: string
        end_date:
          description: Optional end date of the budget
          format: date
          type: string
        filter:
          $ref: '#/components/schemas/BudgetFilter'
        name:
          description: Human-readable name of the budget
          type: string
        period:
          $ref: '#/components/schemas/BudgetPeriod'
        start_date:
          description: Start date of the budget period calculation
          format: date
          type: string
        status_daily:
          description: Amount used in the budget for each day (non-cumulative)
          items:
            properties:
              amount:
                description: Amount used in this day in USD
                example: '123.45'
                type: string
              date:
                format: date
                type: string
            type: object
          type: array
        target_amount:
          description: Target amount of the budget per period in USD
          example: '1234.56'
          type: string
        update_time:
          format: date-time
          type: string
      type: object
    CreateAwsKeyInfo:
      properties:
        key_alias:
          description: The AWS KMS key alias.
          example: alias/projectKey1
          type: string
        key_arn:
          description: The AWS KMS key's Amazon Resource Name (ARN). Note that the
            key's AWS region is inferred from the ARN.
          example: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
          type: string
        reuse_key_for_cluster_volumes:
          description: This field applies only if the `use_cases` property includes
            `STORAGE`. If this is set to `true` or omitted, the key is also used to
            encrypt cluster EBS volumes. To not use this key also for encrypting EBS
            volumes, set this to `false`.
          example: true
          type: boolean
      required:
      - key_arn
      type: object
    CreateCredentialRequest:
      properties:
        aws_credentials:
          properties:
            sts_role:
              properties:
                role_arn:
                  description: The Amazon Resource Name (ARN) of the cross account
                    role.
                  example: arn-aws-iam::111110000000:role/test_role
                  type: string
              type: object
          type: object
        credentials_name:
          description: The human-readable name of the credential configuration object.
          example: credential_1
          maxLength: 256
          minLength: 4
          type: string
      required:
      - credentials_name
      - aws_credentials
      type: object
    CreateCustomerManagedKeyRequest:
      properties:
        aws_key_info:
          $ref: '#/components/schemas/CreateAwsKeyInfo'
        use_cases:
          description: "The cases that the key can be used for. Include one or both\
            \ of these options:\n * `MANAGED_SERVICES`: Encrypts notebook and secret\
            \ data in the control plane\n * `STORAGE`: Encrypts the workspace's root\
            \ S3 bucket (root DBFS and system data) and optionally cluster EBS volumes."
          example:
          - MANAGED_SERVICES
          - STORAGE
          items: string
          type: array
      required:
      - aws_key_info
      - use_cases
      type: object
    CreateLogDeliveryConfigurationParams:
      properties:
        config_name:
          description: The optional human-readable name of the log delivery configuration.
            Defaults to empty.
          maxLength: 255
          minLength: 0
          type: string
        credentials_id:
          description: The ID for a [Databricks credential configuration](#operation/create-credential-config)
            that represents the AWS IAM role with policy and trust relationship as
            described in the main billable usage documentation page. See [Configure
            billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).
          format: uuid
          type: string
        delivery_path_prefix:
          description: The optional delivery path prefix within AWS S3 storage. Defaults
            to empty, which means that logs are delivered to the root of the bucket.
            This must be a valid S3 object key. This must not start or end with a
            slash character.
          type: string
        delivery_start_time:
          description: This field applies only if `log_type` is `BILLABLE_USAGE`.
            This is the optional start month and year for delivery, specified in `YYYY-MM`
            format. Defaults to current year and month.  `BILLABLE_USAGE` logs are
            not available for usage before March 2019 (`2019-03`).
          pattern: 2[0-9][0-9][0-9]-(0[1-9]|1[012])
          type: string
        log_type:
          description: "Log delivery type. Supported values are:\n\n* `BILLABLE_USAGE`\
            \ \u2014 Configure [billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\
            \ For the CSV schema, see the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n\
            \n* `AUDIT_LOGS` \u2014 Configure [audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\
            \ For the JSON schema, see [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)"
          enum:
          - BILLABLE_USAGE
          - AUDIT_LOGS
          type: string
        output_format:
          description: 'The file type of log delivery.


            *  If `log_type` is `BILLABLE_USAGE`, this value must be `CSV`. Only the
            CSV (comma-separated values) format is supported. For the schema, see
            the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html)

            * If `log_type` is `AUDIT_LOGS`, this value must be `JSON`. Only the JSON
            (JavaScript Object Notation) format is supported. For the schema, see
            the [Configuring audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).'
          enum:
          - CSV
          - JSON
          type: string
        status:
          $ref: '#/components/schemas/LogDeliveryConfigStatus'
        storage_configuration_id:
          description: '"The ID for a [Databricks storage configuration](#operation/create-storage-config)  that
            represents the S3 bucket with bucket policy as described in the main billable
            usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html)."'
          format: uuid
          type: string
        workspace_ids_filter:
          description: 'Optional filter of workspace IDs to deliver logs for. By default
            the workspace filter is empty and log delivery applies at the account
            level, delivering workspace level logs for all workspaces in your account,
            plus account level logs. You can optionally set this field to an array
            of workspace IDs (each one is an `int64`) to which log delivery should
            apply to, in which case only workspace level logs relating to the specified
            workspaces will be delivered.

            If you plan to use different log delivery configurations for different
            workspaces, set this field explicitly. Be aware that delivery configurations
            mentioning specific workspaces won''t apply to new workspaces created
            in the future, and delivery won''t include account level logs.

            For some types of Databricks deployments there is only one workspace per
            account ID, so this field is unnecessary.'
          items:
            $ref: '#/components/schemas/WorkspaceId'
          type: array
      type: object
    CreateNetworkRequest:
      properties:
        network_name:
          description: The human-readable name of the network configuration.
          maxLength: 256
          minLength: 4
          type: string
        security_group_ids:
          description: IDs of 1 to 5 security groups associated with this network.
            Security groups IDs **cannot** be used in multiple network configurations.
          items:
            description: ID of security group associated with this network. Security
              group IDs *cannot** be used in multiple network configurations.
            type: string
          maxLength: 5
          minLength: 1
          type: array
        subnet_ids:
          description: IDs of at least 2 subnets associated with this network. Subnet
            IDs **cannot** be used in multiple network configurations.
          items:
            description: ID of subnet associated with this network. Subnet IDs **cannot**
              be used in multiple network configurations.
            type: string
          minLength: 2
          type: array
        vpc_endpoints:
          $ref: '#/components/schemas/NetworkVpcEndpoints'
        vpc_id:
          description: The ID of the VPC associated with this network. VPC IDs can
            be used in multiple network configurations.
          type: string
      required:
      - network_name
      - vpc_id
      - subnet_ids
      - security_group_ids
      type: object
    CreateStorageConfigurationRequest:
      properties:
        root_bucket_info:
          description: Root S3 bucket information.
          properties:
            bucket_name:
              description: The name of the S3 bucket.
              example: test-s3-bucket
              type: string
          type: object
        storage_configuration_name:
          description: The human-readable name of the storage configuration.
          example: storage_conf_1
          maxLength: 256
          minLength: 4
          type: string
      required:
      - storage_configuration_name
      - root_bucket_info
      type: object
    CreateVPCEndpointRequest:
      properties:
        aws_vpc_endpoint_id:
          description: The ID of the VPC endpoint object in AWS.
          type: string
        region:
          description: The AWS region in which this VPC endpoint object exists
          type: string
        vpc_endpoint_name:
          description: The human-readable name of the storage configuration.
          maxLength: 256
          minLength: 4
          type: string
      required:
      - vpc_endpoint_name
      - aws_vpc_endpoint_id
      - region
      type: object
    CreateWorkspaceRequest:
      properties:
        aws_region:
          description: The AWS region of the workspace's Data Plane.
          example: us-west-2
          type: string
        credentials_id:
          description: ID of the workspace's credential configuration object
          example: ccc64f28-ebdc-4c89-add9-5dcb6d7727d8
          format: uuid
          type: string
        deployment_name:
          description: 'The deployment name defines part of the subdomain for the
            workspace. The workspace URL for web application and REST APIs is `<workspace-deployment-name>.cloud.databricks.com`.
            For example, if the deployment name is `abcsales`, your workspace URL
            will be `https://abcsales.cloud.databricks.com`. Hyphens are allowed.  This
            property supports only the set of characters that are allowed in a subdomain.


            If your account has a non-empty deployment name prefix at workspace creation
            time, the workspace deployment name changes so that the beginning has
            the account prefix and a hyphen. For example, if your account''s deployment
            prefix is `acme` and the workspace deployment name is `workspace-1`, the
            `deployment_name` field becomes `acme-workspace-1` and that is the value
            that will be returned in JSON responses for the `deployment_name` field.
            The workspace URL is `acme-workspace-1.cloud.databricks.com`.


            If your account has a non-empty deployment name prefix and you set `deployment_name`
            to the reserved keyword `EMPTY`, `deployment_name` is just the account
            prefix only. For example, if your account''s deployment prefix is `acme`
            and the workspace deployment name is `EMPTY`, `deployment_name` becomes
            `acme` only and the workspace URL is `acme.cloud.databricks.com`.


            Contact your Databricks representatives to add an account deployment name
            prefix to your account. If you do not have a deployment name prefix, the
            special deployment name value `EMPTY` is invalid.


            This value must be unique across all non-deleted deployments across all
            AWS regions.


            If a new workspace omits this property, the server generates a unique
            deployment name for you with the pattern `dbc-xxxxxxxx-xxxx`.'
          example: workspace_1
          pattern: ^(([a-z0-9][a-z0-9-]*[a-z0-9])|([a-z0-9]))|(EMPTY)$
          type: string
        managed_services_customer_managed_key_id:
          description: The ID of the workspace's managed services encryption key configuration
            object. This is used to encrypt the workspace's notebook and secret data
            in the control plane, as well as Databricks SQL queries and query history.
            The provided key configuration object property `use_cases` must contain
            `MANAGED_SERVICES`.
          example: 849b3d6b-e68e-468d-b3e5-deb08b03c56d
          format: uuid
          type: string
        network_id:
          description: The ID of the workspace's network configuration object. To
            use [AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)
            (Public Preview), this field is required.
          example: fd0cc5bc-683c-47e9-b15e-144d7744a496
          format: uuid
          type: string
        pricing_tier:
          description: 'The pricing tier of the workspace. If you do not provide this,
            the API will default to the highest pricing tier available to your account.

            See https://databricks.com/product/aws-pricing for available pricing tier
            information.'
          enum:
          - STANDARD
          - PREMIUM
          - ENTERPRISE
          example: PREMIUM
          type: string
        private_access_settings_id:
          description: 'Only used for PrivateLink, which is in Public Preview. This
            is the ID of the workspace''s private access settings object. This ID
            must be specified for customers using [AWS PrivateLink](https://aws.amazon.com/privatelink/)
            for either front-end (user-to-workspace connection), back-end (data plane
            to control plane connection), or both connection types.


            Before configuring PrivateLink, it is important to read the [Databricks
            article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).'
          format: uuid
          type: string
        storage_configuration_id:
          description: The ID of the workspace's storage configuration object.
          example: b43a6064-04c1-4e1c-88b6-d91e5b136b13
          format: uuid
          type: string
        storage_customer_managed_key_id:
          description: The ID of the workspace's storage encryption key configuration
            object. This is used to encrypt the workspace's root S3 bucket (root DBFS
            and system data) and optionally cluster EBS volumes. The provided key
            configuration object property `use_cases` must contain `STORAGE`.
          format: uuid
          type: string
        workspace_name:
          description: The workspace's human-readable name.
          example: My workspace 1
          type: string
      required:
      - workspace_name
      - aws_region
      - credentials_id
      - storage_configuration_id
      type: object
    Credential:
      properties:
        account_id:
          description: The Databricks account ID that hosts the credential.
          format: uuid
          type: string
        aws_credentials:
          properties:
            sts_role:
              properties:
                external_id:
                  description: The external ID that needs to be trusted by the cross-account
                    role. This is always your Databricks account ID.
                  type: string
                role_arn:
                  description: The Amazon Resource Name (ARN) of the cross-account
                    role.
                  type: string
              type: object
          type: object
        creation_time:
          description: Time in epoch milliseconds when the credential was created.
          format: int64
          type: integer
        credentials_id:
          description: Databricks credential configuration ID.
          format: uuid
          type: string
        credentials_name:
          description: The human-readable name of the credential configuration object.
          maxLength: 256
          minLength: 4
          type: string
      type: object
    CustomerManagedKey:
      properties:
        account_id:
          description: The Databricks account ID that holds the customer-managed key.
          format: uuid
          type: string
        aws_key_info:
          $ref: '#/components/schemas/AwsKeyInfo'
        creation_time:
          description: Time in epoch milliseconds when the customer key was created.
          format: int64
          type: integer
        customer_managed_key_id:
          description: ID of the encryption key configuration object.
          format: uuid
          type: string
        use_cases:
          description: "The cases that the key can be used for. Include one or both\
            \ of these options:\n * `MANAGED_SERVICES`: Encrypts notebook and secret\
            \ data in the control plane\n * `STORAGE`: Encrypts the workspace's root\
            \ S3 bucket (root DBFS and system data) and optionally cluster EBS volumes."
          example:
          - MANAGED_SERVICES
          - STORAGE
          items: string
          type: array
      type: object
    Error:
      properties:
        message:
          description: Cause of the error
          type: string
      type: object
    ListCredentialsResponse:
      description: List of credential configuration objects.
      items:
        $ref: '#/components/schemas/Credential'
      type: array
    ListCustomerManagedKeysResponse:
      description: Array of key configuration objects.
      items:
        $ref: '#/components/schemas/CustomerManagedKey'
      type: array
    ListNetworksResponse:
      description: Array of network configuration objects.
      items:
        $ref: '#/components/schemas/Network'
      type: array
    ListPrivateAccessSettingsResponse:
      description: Private access settings objects.
      items:
        $ref: '#/components/schemas/PrivateAccessSettings'
      type: array
    ListStorageConfigurationsResponse:
      description: Storage configuration objects.
      items:
        $ref: '#/components/schemas/StorageConfiguration'
      type: array
    ListVPCEndpointsResponse:
      description: List VPC endpoint configurations.
      items:
        $ref: '#/components/schemas/VPCEndpoint'
      type: array
    ListWorkspaceEncryptionKeyRecordsResponse:
      properties:
        workspaceEncryptionKeyRecords:
          items:
            $ref: '#/components/schemas/WorkspaceEncryptionKeyRecord'
          type: array
      type: object
    ListWorkspacesResponse:
      description: An array of workspaces.
      items:
        $ref: '#/components/schemas/Workspace'
      type: array
    LogDeliveryConfigStatus:
      description: Status of log delivery configuration. Set to `ENABLED` (enabled)
        or `DISABLED` (disabled). Defaults to `ENABLED`. You can [enable or disable
        the configuration](#operation/patch-log-delivery-config-status) later. Deletion
        of a configuration is not supported, so disable a log delivery configuration
        that is no longer needed.
      enum:
      - ENABLED
      - DISABLED
      type: string
    LogDeliveryConfiguration:
      allOf:
      - $ref: '#/components/schemas/CreateLogDeliveryConfigurationParams'
      properties:
        account_id:
          description: The Databricks account ID that hosts the log delivery configuration.
          format: uuid
          type: string
        config_id:
          description: Databricks log delivery configuration ID.
          format: uuid
          type: string
        creation_time:
          description: Time in epoch milliseconds when the log delivery configuration
            was created.
          format: int64
          type: integer
        log_delivery_status:
          description: Databricks log delivery status.
          properties:
            last_attempt_time:
              description: The UTC time for the latest log delivery attempt.
              format: date-time
              type: string
            last_successful_attempt_time:
              description: The UTC time for the latest successful log delivery.
              format: date-time
              type: string
            message:
              description: Informative message about the latest log delivery attempt.
                If the log delivery fails with USER_FAILURE, error details will be
                provided for fixing misconfigurations in cloud permissions.
              type: string
            status:
              description: 'The status string for log delivery. Possible values are:
                * `CREATED`: There were no log delivery attempts since the config
                was created. * `SUCCEEDED`: The latest attempt of log delivery has
                succeeded completely. * `USER_FAILURE`: The latest attempt of log
                delivery failed because of misconfiguration of customer provided permissions
                on role or storage. * `SYSTEM_FAILURE`: The latest attempt of log
                delivery failed because of an Databricks internal error. Contact support
                if it doesn''t go away soon. * `NOT_FOUND`: The log delivery status
                as the configuration has been disabled since the release of this feature
                or there are no workspaces in the account.'
              enum:
              - CREATED
              - SUCCEEDED
              - USER_FAILURE
              - SYSTEM_FAILURE
              - NOT_FOUND
              type: string
          type: object
        update_time:
          description: Time in epoch milliseconds when the log delivery configuration
            was updated.
          format: int64
          type: integer
      type: object
    Network:
      properties:
        account_id:
          description: The Databricks account ID associated with this network configuration.
          format: uuid
          type: string
        creation_time:
          description: Time in epoch milliseconds when the network was created.
          format: int64
          type: integer
        error_messages:
          description: Array of error messages about the network configuration.
          items:
            $ref: '#/components/schemas/NetworkHealth'
          type: array
        network_id:
          description: The Databricks network configuration ID.
          format: uuid
          type: string
        network_name:
          description: The human-readable name of the network configuration.
          maxLength: 256
          minLength: 4
          type: string
        security_group_ids:
          items:
            description: ID of a security group associated with this network configuration.
              Security group IDs **cannot** be used in multiple networks.
            type: string
          maxLength: 5
          minLength: 1
          type: array
        subnet_ids:
          items:
            description: The ID of a subnet associated with this network configuration.
              Subnet IDs **cannot** be used in multiple network configurations.
            type: string
          minLength: 2
          type: array
        vpc_endpoints:
          $ref: '#/components/schemas/NetworkVpcEndpoints'
        vpc_id:
          description: The ID of the VPC associated with this network configuration.
            VPC IDs can be used in multiple networks.
          type: string
        vpc_status:
          description: "The status of this network configuration object in terms of\
            \ its use in a workspace:\n\n* `UNATTACHED`: Unattached.\n\n * `VALID`:\
            \ Valid.\n\n * `BROKEN`: Broken.\n\n * `WARNED`: Warned."
          enum:
          - UNATTACHED
          - VALID
          - BROKEN
          - WARNED
          type: string
        warning_messages:
          description: Array of warning messages about the network configuration.
          items:
            $ref: '#/components/schemas/NetworkWarning'
          type: array
        workspace_id:
          description: Workspace ID associated with this network configuration.
          format: int64
          type: integer
      type: object
    NetworkHealth:
      properties:
        error_message:
          description: Details of the error.
          type: string
        error_type:
          description: 'The AWS resource associated with this error: credentials,
            VPC, subnet, security group, or network ACL.'
          enum:
          - credentials
          - vpc
          - subnet
          - securityGroup
          - networkAcl
          type: string
      type: object
    NetworkVpcEndpoints:
      description: If specified, contains the VPC endpoints used to allow cluster
        communication from this VPC over [AWS PrivateLink](https://aws.amazon.com/privatelink/).
      properties:
        dataplane_relay:
          description: 'The VPC endpoint ID used by this Network to access the Databricks
            secure cluster connectivity relay. See [Secure Cluster Connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html).


            This is a list type for future compatibility, but currently only one VPC
            endpoint ID should be supplied.


            Note: This is the Databricks-specific ID of the VPC endpoint object in
            the Account API, not the AWS VPC endpoint ID that you see for your endpoint
            in the AWS Console.'
          items:
            format: uuid
            type: string
        rest_api:
          description: 'The VPC endpoint ID used by this Network to access the Databricks
            REST API. Databricks clusters make calls to our REST API as part of cluster
            creation, mlflow tracking, and many other features. Thus, this is required
            even if your workspace allows public access to the REST API.


            This is a list type for future compatibility, but currently only one VPC
            endpoint ID should be supplied.


            Note: This is the Databricks-specific ID of the VPC endpoint object in
            the Account API, not the AWS VPC endpoint ID that you see for your endpoint
            in the AWS Console.'
          items:
            format: uuid
            type: string
      required:
      - rest_api
      - dataplane_relay
      type: object
    NetworkWarning:
      properties:
        warning_message:
          description: Details of the warning.
          type: string
        warning_type:
          description: 'The AWS resource associated with this warning: a subnet or
            a security group.'
          enum:
          - subnet
          - securityGroup
          type: string
      type: object
    PrivateAccessSettings:
      properties:
        account_id:
          description: The Databricks account ID that hosts the credential.
          format: uuid
          type: string
        allowed_vpc_endpoint_ids:
          description: 'An array of Databricks VPC endpoint IDs. This is the Databricks
            ID returned when registering the VPC endpoint configuration in your Databricks
            account. This is _not_ the ID of the VPC endpoint in AWS.


            Only used when `private_access_level` is set to `ENDPOINT`. This is an
            allow list of VPC endpoints registered in your Databricks account that
            can connect to your workspace over AWS PrivateLink.


            Note: if hybrid access to your workspace is enabled by setting `public_access_enabled`
            to `true`, then this control only works for PrivateLink connections. To
            control how your workspace is accessed via public internet, see the article
            on [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).'
          items:
            format: uuid
            type: string
          type: array
        private_access_level:
          description: 'The private access level controls which VPC endpoints can
            connect to the UI or API of any workspace that attaches this private access
            settings object.

            * `ANY` (the default): Any VPC endpoint can connect to your workspace.

            * `ACCOUNT` level access lets only VPC endpoints that are registered in
            your Databricks account connect to your workspace.

            * `ENDPOINT` level access lets only specified VPC endpoints connect to
            your workspace. See the `allowed_vpc_endpoint_ids` for details.'
          enum:
          - ANY
          - ACCOUNT
          - ENDPOINT
          example: ENDPOINT
          type: string
        private_access_settings_id:
          description: Databricks private access settings ID.
          format: uuid
          type: string
        private_access_settings_name:
          description: The human-readable name of the private access settings object.
          maxLength: 256
          minLength: 4
          type: string
        public_access_enabled:
          description: Determines if the workspace can be accessed over public internet.
            For fully private workspaces, you can optionally specify `false`, but
            only if you implement both the front-end and the back-end PrivateLink
            connections. Otherwise, specify `true`, which means that public access
            is still enabled.
          type: boolean
        region:
          description: The AWS region for workspaces attached to this private access
            settings object.
          type: string
      type: object
    StorageConfiguration:
      properties:
        account_id:
          description: The Databricks account ID that hosts the credential.
          format: uuid
          type: string
        creation_time:
          description: Time in epoch milliseconds when the storage configuration was
            created.
          format: int64
          type: integer
        root_bucket_info:
          description: Root S3 bucket information.
          properties:
            bucket_name:
              description: The name of the S3 bucket.
              type: string
          type: object
        storage_configuration_id:
          description: Databricks storage configuration ID.
          format: uuid
          type: string
        storage_configuration_name:
          description: The human-readable name of the storage configuration.
          maxLength: 256
          minLength: 4
          type: string
      type: object
    UpdateLogDeliveryConfigurationStatusRequest:
      properties:
        status:
          $ref: '#/components/schemas/LogDeliveryConfigStatus'
      required:
      - status
      type: object
    UpdateWorkspaceRequest:
      properties:
        aws_region:
          description: The AWS region of the workspace's Data Plane. For example,
            `us-west-2`. This parameter is available only for updating failed workspaces.
          example: us-west-2
          type: string
        credentials_id:
          description: ID of the workspace's credential configuration object. This
            parameter is available for updating both failed and running workspaces.
          format: uuid
          type: string
        managed_services_customer_managed_key_id:
          description: The ID of the workspace's managed services encryption key configuration
            object. This parameter is available only for updating failed workspaces.
          format: uuid
          type: string
        network_id:
          description: 'The ID of the workspace''s network configuration object. Used
            only if you already use a customer-managed VPC. This change is supported
            only if you specified a network configuration ID when the workspace was
            created. In other words, you cannot switch from a Databricks-managed VPC
            to a customer-managed VPC. This parameter is available for updating both
            failed and running workspaces. Note: You cannot use a network configuration
            update in this API to add support for PrivateLink (in Public Preview).
            To add PrivateLink to an existing workspace, contact your Databricks representative.'
          format: uuid
          type: string
        storage_configuration_id:
          description: The ID of the workspace's storage configuration object. This
            parameter is available only for updating failed workspaces.
          format: uuid
          type: string
        storage_customer_managed_key_id:
          description: The ID of the key configuration object for workspace storage.
            This parameter is available for updating both failed and running workspaces.
          format: uuid
          type: string
      type: object
    UpsertPrivateAccessSettingsRequest:
      properties:
        allowed_vpc_endpoint_ids:
          description: 'An array of Databricks VPC endpoint IDs. This is the Databricks
            ID that is returned when registering the VPC endpoint configuration in
            your Databricks account. This is not the ID of the VPC endpoint in AWS.


            Only used when `private_access_level` is set to `ENDPOINT`. This is an
            allow list of VPC endpoints that in your account that can connect to your
            workspace over AWS PrivateLink.


            If hybrid access to your workspace is enabled by setting `public_access_enabled`
            to `true`, then this control only works for PrivateLink connections. To
            control how your workspace is accessed via public internet, see the article
            for [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).'
          items:
            format: uuid
            type: string
          type: array
        private_access_level:
          default: ANY
          description: 'The private access level controls which VPC endpoints can
            connect to the UI or API of any workspace that attaches this private access
            settings object.

            * `ANY` level access lets any VPC endpoint connect to your workspace.

            * `ACCOUNT` level access lets only VPC endpoints that are registered in
            your Databricks account connect to your workspace.

            * `ENDPOINT` level access lets only specified VPC endpoints connect to
            your workspace. Please see the `allowed_vpc_endpoint_ids` documentation
            for more details.'
          enum:
          - ANY
          - ACCOUNT
          - ENDPOINT
          example: ENDPOINT
          type: string
        private_access_settings_name:
          description: The human-readable name of the private access settings object.
          maxLength: 256
          minLength: 4
          type: string
        public_access_enabled:
          default: false
          description: Determines if the workspace can be accessed over public internet.
            For fully private workspaces, you can optionally specify `false`, but
            only if you implement both the front-end and the back-end PrivateLink
            connections. Otherwise, specify `true`, which means that public access
            is still enabled.
          type: boolean
        region:
          description: The AWS region for workspaces associated with this private
            access settings object. This must be a [region that Databricks supports
            for PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).
          type: string
      required:
      - private_access_settings_name
      - region
      type: object
    UsageDownloadMonth:
      description: Format specification for month in the format `YYYY-MM`. This is
        used to specify billable usage `start_month` and `end_month` properties. Note
        that billable usage logs are unavailable before March 2019 (`2019-03`).
      pattern: 2[0-9][0-9][0-9]-(0[1-9]|1[012])
      type: string
    VPCEndpoint:
      properties:
        account_id:
          description: The Databricks account ID that hosts the VPC endpoint configuration.
          format: uuid
          type: string
        aws_account_id:
          description: The AWS Account in which the VPC endpoint object exists.
          type: string
        aws_endpoint_service_id:
          description: The ID of the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html)
            that this VPC endpoint is connected to. Please find the list of endpoint
            service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).
          type: string
        aws_vpc_endpoint_id:
          description: The ID of the VPC endpoint object in AWS.
          type: string
        region:
          description: The AWS region in which this VPC endpoint object exists.
          type: string
        state:
          description: The current state (such as `available` or `rejected`) of the
            VPC endpoint. Derived from AWS. For the full set of values, see [AWS DescribeVpcEndpoint
            documentation](https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-vpc-endpoints.html)
            for details.
          type: string
        use_case:
          description: 'This enumeration represents the type of Databricks VPC [endpoint
            service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html)
            that was used when creating this VPC endpoint.


            If the VPC endpoint connects to the Databricks control plane for either
            the front-end connection or the back-end REST API connection, the value
            is `WORKSPACE_ACCESS`.


            If the VPC endpoint connects to the Databricks workspace for the back-end
            [secure cluster connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html)
            relay, the value is `DATAPLANE_RELAY_ACCESS`.'
          enum:
          - WORKSPACE_ACCESS
          - DATAPLANE_RELAY_ACCESS
          type: string
        vpc_endpoint_id:
          description: Databricks VPC endpoint ID. This is the Databricks-specific
            name of the VPC endpoint. Do not confuse this with the `aws_vpc_endpoint_id`,
            which is the ID within AWS of the VPC endpoint.
          format: uuid
          type: string
        vpc_endpoint_name:
          description: The human-readable name of the storage configuration.
          maxLength: 256
          minLength: 4
          type: string
      type: object
    Workspace:
      properties:
        account_id:
          description: Databricks account ID
          format: uuid
          type: string
        aws_region:
          description: The AWS region of the workspace Data Plane. For example, `us-west-2`.
          type: string
        creation_time:
          description: Time in epoch milliseconds when the workspace was created.
          format: int64
          type: integer
        credentials_id:
          description: ID of the workspace's credential configuration object.
          format: uuid
          type: string
        deployment_name:
          description: 'The deployment name defines part of the subdomain for the
            workspace. The workspace URL for web application and REST APIs is `<deployment-name>.cloud.databricks.com`.


            This value must be unique across all non-deleted deployments across all
            AWS regions.'
          maxLength: 64
          type: string
        managed_services_customer_managed_key_id:
          description: ID of the key configuration for encrypting managed services.
          format: uuid
          type: string
        pricing_tier:
          description: 'The pricing tier of the workspace.

            See https://databricks.com/product/aws-pricing for available pricing tier
            information.'
          enum:
          - UNKNOWN
          - COMMUNITY_EDITION
          - STANDARD
          - PREMIUM
          - ENTERPRISE
          - DEDICATED
          example: PREMIUM
          type: string
        private_access_settings_id:
          description: 'Only used for PrivateLink, which is in Public Preview. This
            is the ID of the workspace''s private access settings object. This ID
            must be specified for customers using [AWS PrivateLink](https://aws.amazon.com/privatelink/)
            for either front-end (user-to-workspace connection), back-end (data plane
            to control plane connection), or both connection types.


            Before configuring PrivateLink, it is important to read the [Databricks
            article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).'
          format: uuid
          type: string
        storage_configuration_id:
          description: ID of the workspace's storage configuration object.
          format: uuid
          type: string
        storage_customer_managed_key_id:
          description: ID of the key configuration for encrypting workspace storage.
          format: uuid
          type: string
        workspace_id:
          description: Workspace ID.
          format: int64
          type: integer
        workspace_name:
          description: The human-readable name of the workspace.
          maxLength: 100
          minLength: 1
          type: string
        workspace_status:
          description: 'The status of the workspace.

            For workspace creation, it is typically initially `PROVISIONING`. Continue
            to check the status until the status is `RUNNING`. For detailed instructions
            of creating a new workspace with this API **including error handling**
            see [Create a new workspace using the Account Management API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).'
          enum:
          - NOT_PROVISIONED
          - PROVISIONING
          - RUNNING
          - FAILED
          - BANNED
          - CANCELLING
          example: RUNNING
          type: string
        workspace_status_message:
          description: Message describing the current workspace status.
          example: Workspace resources are being set up.
          type: string
      type: object
    WorkspaceEncryptionKeyRecord:
      properties:
        aws_key_info:
          $ref: '#/components/schemas/AwsKeyInfo'
        customer_managed_key_id:
          description: ID of the encryption key configuration object.
          format: uuid
          type: string
        key_status:
          enum:
          - UNKNOWN
          - ATTACHED
          - DETACHED
          example: ATTACHED
          type: string
        record_id:
          description: ID for the workspace-key mapping record
          format: uuid
          type: string
        update_time:
          description: Time in epoch milliseconds when the record was added.
          format: int64
          type: integer
        use_case:
          description: "Possible values are:\n - `MANAGED_SERVICES`: Encrypts notebook\
            \ and secret data in the control plane\n - `STORAGE`: Encrypts the workspace's\
            \ root S3 bucket (root DBFS and system data) and optionally cluster EBS\
            \ volumes."
          enum:
          - MANAGED_SERVICES
          - STORAGE
          example: STORAGE
          type: string
        workspace_id:
          description: Workspace ID.
          format: int64
          type: integer
      type: object
    WorkspaceId:
      format: int64
      type: integer
    WrappedCreateLogDeliveryConfiguration:
      properties:
        log_delivery_configuration:
          allOf:
          - $ref: '#/components/schemas/CreateLogDeliveryConfigurationParams'
          required:
          - log_type
          - output_format
          - credentials_id
          - storage_configuration_id
      type: object
    WrappedLogDeliveryConfiguration:
      properties:
        log_delivery_configuration:
          allOf:
          - $ref: '#/components/schemas/LogDeliveryConfiguration'
      type: object
    WrappedLogDeliveryConfigurations:
      properties:
        log_delivery_configurations:
          items:
            $ref: '#/components/schemas/LogDeliveryConfiguration'
          type: array
      type: object
  securitySchemes:
    basicAuth:
      scheme: basic
      type: http
info:
  description: "The Account API offers centralized management of multiple Databricks\
    \ workspaces and related resources.\n\nUse this API for the following tasks:\n\
    \n* **Create new workspaces** \u2014 This feature is available if your account\
    \ is on the [E2 version of the platform](https://docs.databricks.com/getting-started/overview.html#e2-architecture)\
    \ or on a select custom plan that allows multiple workspaces per account.\n\n\
    * **Configure log delivery (billable usage or audit logs)** \u2014 This feature\
    \ is Public Preview and works with all account IDs.\n\n* **Download billable usage\
    \ logs** \u2014 This feature works with all account IDs.\n\n### New workspaces\n\
    \nUse this API to programmatically deploy, update, and delete workspaces. All\
    \ workspaces have associated cloud credential configurations and storage configurations.\
    \ This feature is available if your account is on the E2 version of the platform\
    \ or on a select custom plan that allows multiple workspaces per account. For\
    \ additional details and steps, see [Create a new workspace using the Account\
    \ API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n\
    \nThere are additional optional features that you can associate with your workspace\
    \ for deployment types and subscription types that support them. If you use these\
    \ features you will create additional configurations with this API. If you have\
    \ questions about availability, contact your Databricks representative:\n\n* \
    \ **[Customer-managed VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html)**\
    \ \u2014 Provide your own VPC. See the note below about regions for VPCs. To configure\
    \ your workspace to use [AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\
    \ ([Public Preview](http://docs.databricks.com/release-notes/release-types.html))\
    \ for any type of connection, it is required that your workspace use a customer-managed\
    \ VPC.\n\n*  **[Secure cluster connectivity](http://docs.databricks.com/security/secure-cluster-connectivity.html)**\
    \ \u2014 Network architecture with no VPC open ports and no Databricks runtime\
    \ worker public IP addresses. This is the default in workspaces for accounts on\
    \ the E2 version of the platform as of September 1, 2020.\n\n* **[Customer-managed\
    \ keys for managed services](http://docs.databricks.com/security/keys/customer-managed-keys-managed-services-aws.html)**\
    \ \u2014 (Public Preview) Provide KMS keys to encrypt notebook and secret data\
    \ in the control plane, as well as Databricks SQL queries and query history. This\
    \ feature requires the Enterprise pricing tier and is available only for [some\
    \ AWS regions](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).\n\
    \n* **[Customer-managed keys for storage](https://docs.databricks.com/security/keys/customer-managed-keys-storage-aws.html)**\
    \ \u2014 (Public Preview) Provide KMS keys to encrypt the workspace's S3 bucket\
    \ in the customer's AWS account, which contains the DBFS root and system data,\
    \ as well as optionally encrypting cluster EBS instances. This feature requires\
    \ the Enterprise pricing tier and is available only for [some AWS regions](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).\
    \ \n\n* **[AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)**\
    \ - ([Public Preview](http://docs.databricks.com/release-notes/release-types.html))\
    \ AWS PrivateLink provides private connectivity between VPCs, AWS services, and\
    \ on-premises networks, without exposing the traffic to the public Internet. Workspaces\
    \ on the E2 version of the platform support adding PrivateLink connections for\
    \ two connection types. A front-end PrivateLink connection configure users to\
    \ connect to the <Databricks> web application or REST API over a PrivateLink endpoint.\
    \ A back-end PrivateLink connection configures Databricks Runtime clusters connect\
    \ to the control plane using two VPC endpoints (one for REST APIs, one for the\
    \ secure cluster connectivity relay). PrivateLink is available only for some AWS\
    \ regions. Your account must be enabled for PrivateLink to use these APIs. Before\
    \ configuring PrivateLink, it is important to read the [Databricks article about\
    \ PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\
    \ You can use the workspace creation API to add PrivateLink to a new workspace.\
    \ If you want to add PrivateLink to an existing workspace, for the final workspace\
    \ configuration update, you cannot use this API. Instead, contact your Databricks\
    \ representative to make the change.\n\nTo create a new workspace, use the [Create\
    \ a workspace](#operation/create-workspace) operation. For detailed instructions\
    \ of creating a new workspace with this API, see [Create a new workspace using\
    \ the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n\
    \nTo update an existing workspace, use the [Update a workspace](#operation/patch-workspace)\
    \ operation. You can update only specific fields, and the set varies depending\
    \ on whether it is a failed workspace or a running workspace.\n### Log delivery\n\
    \nUse this API to manage log delivery. It supports two log types:\n* `BILLABLE_USAGE`\
    \ \u2014 Learn more at [Configure billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\
    \ For the CSV schema, see the [Usage page](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n\
    \n* `AUDIT_LOGS` \u2014 Learn more at [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html),\
    \ including the JSON schema.\n\nFor the API reference, see [log delivery configurations](#tag/Log-delivery-configurations).\n\
    ### Download Billable Usage Logs\n\nUse this API to download a CSV file that contains\
    \ billable usage logs for the specified account and date range.\n\nTo programmatically\
    \ process and analyze billable usage, Databricks recommends that you configure\
    \ continuous billable usage delivery to an S3 bucket as documented on the [Deliver\
    \ and access billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html)\
    \ page. You can also use the Account API to directly download billable usage.\n\
    \nSee [Billable Usage Download](#tag/Billable-usage-download) for the API reference.\n\
    ### Configure Budgets\n\nUse this API to configure usage budgets for workspaces\
    \ in your account, set up notifications for exceeding budget and query budget\
    \ status.\n# Authentication\nThe Account API is different from most Databricks\
    \ REST APIs:\n* It is published on `accounts.cloud.databricks.com`, not on the\
    \ domain that matches your workspace URL.\n* Authenticate with your account name\
    \ and password on the API, rather than a personal access token. For details, see\
    \ [Create a new workspace using the Account API](https://docs.databricks.com/administration-guide/account-api/new-workspace.html). "
  title: Account API
  version: 2.0.0
openapi: 3.0.0
paths:
  /accounts/{account_id}/budget:
    get:
      description: Get all budgets associated with this account, including non-cumulative
        status for each day the budget is configured for.
      operationId: get-budgets
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BudgetList'
          description: The list of budgets was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all budgets associated with this account
      tags:
      - Budgets
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: Create a new budget in this account.
      operationId: create-budget
      requestBody:
        content:
          application/json:
            schema:
              properties:
                budget:
                  $ref: '#/components/schemas/BudgetCreateRequest'
              required:
              - budget
              type: object
        description: Properties of the new budget
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BudgetWithStatus'
          description: The budget was successfully created.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create a new budget
      tags:
      - Budgets
  /accounts/{account_id}/budget/{budget_id}:
    delete:
      description: Delete budget specified by its UUID.
      operationId: delete-budget
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BudgetWithStatus'
          description: The budget that was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete budget
      tags:
      - Budgets
    get:
      description: Get budget specified by its UUID, including non-cumulative status
        for each day the budget is configured for.
      operationId: get-budget
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/BudgetWithStatus'
          description: The budget was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get budget and its status
      tags:
      - Budgets
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Budget ID
      in: path
      name: budget_id
      required: true
      schema:
        format: uuid
        type: string
    patch:
      description: Modify a budget in this account. Budget properties will be fully
        overwritten.
      operationId: modify-budget
      requestBody:
        content:
          application/json:
            schema:
              properties:
                budget:
                  $ref: '#/components/schemas/BudgetCreateRequest'
              required:
              - budget
              type: object
        description: Properties to set the budget to
        required: true
      summary: Modify a budget
      tags:
      - Budgets
  /accounts/{account_id}/credentials:
    get:
      description: Get all Databricks credential configurations associated with an
        account specified by ID.
      operationId: get-credential-configs
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListCredentialsResponse'
          description: Credential configurations were returned successfully.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all credential configurations
      tags:
      - Credential configurations
    parameters:
    - $ref: '#/components/parameters/account_id_dual_use'
    post:
      description: 'Create a Databricks credential configuration that represents cloud
        cross-account credentials for a specified account. Databricks uses this to
        set up network infrastructure properly to host Databricks clusters. For your
        AWS IAM role, you need to trust the External ID (the Databricks Account API
        account ID)  in the returned credential object, and configure the required
        access policy.


        Save the response''s `credentials_id` field, which is the ID for your new
        credential configuration object.


        For detailed instructions of creating a new workspace with this API, see [Create
        a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)'
      operationId: create-credential-config
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCredentialRequest'
        description: Properties of the new credential configuration.
        required: true
      responses:
        '201':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Credential'
          description: The credential configuration creation request succeeded.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create credential configuration
      tags:
      - Credential configurations
  /accounts/{account_id}/credentials/{credentials_id}:
    delete:
      description: Delete a Databricks credential configuration object for an account,
        both specified by ID. You cannot delete a credential that is associated with
        any workspace.
      operationId: delete-credential-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Credential'
          description: The credential configuration was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete credential configuration
      tags:
      - Credential configurations
    get:
      description: Get a Databricks credential configuration object for an account,
        both specified by ID.
      operationId: get-credential-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Credential'
          description: The credential configuration was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get credential configuration
      tags:
      - Credential configurations
    parameters:
    - $ref: '#/components/parameters/account_id_dual_use'
    - description: Databricks Account API credential configuration ID
      in: path
      name: credentials_id
      required: true
      schema:
        format: uuid
        type: string
  /accounts/{account_id}/customer-managed-key-history:
    get:
      description: 'Get a list of records of how key configurations were associated
        with workspaces.


        **Important**: Customer-managed keys are supported only for some deployment
        types, subscription types, and AWS regions.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-key-workspace-history
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListWorkspaceEncryptionKeyRecordsResponse'
          description: The key's workspace association history was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get history of a key's associations with workspaces
      tags:
      - Key configurations
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
  /accounts/{account_id}/customer-managed-keys:
    get:
      description: 'Get all customer-managed key configuration objects for an account.
        If the key is specified as a workspace''s managed services customer-managed
        key, Databricks will use the key to encrypt the workspace''s notebooks and
        secrets in the control plane, as well as Databricks SQL queries and query
        history. If the key is specified as a workspace''s storage customer-managed
        key, the key is used to encrypt the workspace''s root S3 bucket and optionally
        can encrypt cluster EBS volumes data in the data plane.


        **Important**: Customer-managed keys are supported only for some deployment
        types, subscription types, and AWS regions.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-key-configs
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListCustomerManagedKeysResponse'
          description: The encryption key configurations were successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all encryption key configurations
      tags:
      - Key configurations
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: 'Create a customer-managed key configuration object for an account,
        specified by ID. This operation uploads a reference to a customer-managed
        key to Databricks. If the key is assigned as a workspace''s customer-managed
        key for managed services, Databricks uses the key to encrypt the workspaces
        notebooks and secrets in the control plane, as well as Databricks SQL queries
        and query history. If it is specified as a workspace''s customer-managed key
        for workspace storage, the key encrypts the workspace''s root S3 bucket (which
        contains the workspace''s root DBFS and system data) and optionally cluster
        EBS volume data.


        **Important**: Customer-managed keys are supported only for some deployment
        types, subscription types, and AWS regions.


        This operation is available only if your account is on the E2 version of the
        platform or on a select custom plan that allows multiple workspaces per account.'
      operationId: create-key-config
      requestBody:
        content:
          application/json:
            examples:
              All:
                value:
                  aws_key_info:
                    key_alias: alias/projectKey
                    key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                    reuse_key_for_cluster_volumes: true
                  use_cases:
                  - MANAGED_SERVICES
                  - STORAGE
              Managed Services:
                value:
                  aws_key_info:
                    key_alias: alias/projectKey
                    key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                  use_cases:
                  - MANAGED_SERVICES
              Storage:
                value:
                  aws_key_info:
                    key_alias: alias/projectKey
                    key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                    reuse_key_for_cluster_volumes: false
                  use_cases:
                  - STORAGE
            schema:
              $ref: '#/components/schemas/CreateCustomerManagedKeyRequest'
        description: Properties of the encryption key configuration.
        required: true
      responses:
        '201':
          content:
            application/json:
              examples:
                All:
                  value:
                    account_id: 449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65
                    aws_key_info:
                      key_alias: alias/projectKey
                      key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                      key_region: us-west-2
                      reuse_key_for_cluster_volumes: true
                    creation_time: 0
                    customer_managed_key_id: 680290f4-6931-497c-a6b4-514f6694e228
                    use_cases:
                    - MANAGED_SERVICES
                    - STORAGE
                Managed Services:
                  value:
                    account_id: 449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65
                    aws_key_info:
                      key_alias: alias/projectKey
                      key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                      key_region: us-west-2
                    creation_time: 0
                    customer_managed_key_id: 680290f4-6931-497c-a6b4-514f6694e228
                    use_cases:
                    - MANAGED_SERVICES
                Storage:
                  value:
                    account_id: 449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65
                    aws_key_info:
                      key_alias: alias/projectKey
                      key_arn: arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321
                      key_region: us-west-2
                      reuse_key_for_cluster_volumes: false
                    creation_time: 0
                    customer_managed_key_id: 680290f4-6931-497c-a6b4-514f6694e228
                    use_cases:
                    - STORAGE
              schema:
                $ref: '#/components/schemas/CustomerManagedKey'
          description: The encryption key configuration was successfully created.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '403':
          $ref: '#/components/responses/Forbidden'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create encryption key configuration
      tags:
      - Key configurations
  /accounts/{account_id}/customer-managed-keys/{customer_managed_key_id}:
    delete:
      description: Delete a customer-managed key configuration object for an account.
        You cannot delete a configuration that is associated with a running workspace.
      operationId: delete-key-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CustomerManagedKey'
          description: The encryption key configuration was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete encryption key configuration
      tags:
      - Key configurations
    get:
      description: 'Get a customer-managed key configuration object for an account,
        specified by ID. This operation uploads a reference to a customer-managed
        key to Databricks. If assigned as a workspace''s customer-managed key for
        managed services, Databricks uses the key to encrypt the workspaces notebooks
        and secrets in the control plane, as well as Databricks SQL queries and query
        history. If it is specified as a workspace''s customer-managed key for storage,
        the key encrypts the workspace''s root S3 bucket (which contains the workspace''s
        root DBFS and system data) and optionally cluster EBS volume data.


        **Important**: Customer-managed keys are supported only for some deployment
        types, subscription types, and AWS regions.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-key-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CustomerManagedKey'
          description: The encryption key configuration was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get encryption key configuration
      tags:
      - Key configurations
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Databricks encryption key configuration ID.
      in: path
      name: customer_managed_key_id
      required: true
      schema:
        format: uuid
        type: string
  /accounts/{account_id}/log-delivery:
    get:
      description: Get all Databricks log delivery configurations associated with
        an account specified by ID.
      operationId: get-log-delivery-configs
      parameters:
      - description: Filter by status `ENABLED` or `DISABLED`.
        in: query
        name: status
        schema:
          $ref: '#/components/schemas/LogDeliveryConfigStatus'
      - description: Filter by credential configuration ID.
        in: query
        name: credentials_id
        schema:
          type: string
      - description: Filter by storage configuration ID.
        in: query
        name: storage_configuration_id
        schema:
          type: string
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WrappedLogDeliveryConfigurations'
          description: Log delivery configurations were returned successfully.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all log delivery configurations
      tags:
      - Log delivery configurations
    parameters:
    - $ref: '#/components/parameters/account_id'
    post:
      description: 'Create a new Databricks log delivery configuration to enable delivery
        of the specified type of logs to your storage location. This requires that
        you already created a [credential object](#operation/create-credential-config)
        (which encapsulates a cross-account service IAM role) and a [storage configuration
        object](#operation/create-storage-config) (which encapsulates an S3 bucket).


        For full details, including the required IAM role policies and bucket policies,
        see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html)
        or [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).


        Note: There is a limit on the number of log delivery configurations available
        per account (each limit applies separately to each log type including billable
        usage and audit logs). You can create a maximum of two enabled account-level
        delivery configurations (configurations without a workspace filter) per type.
        Additionally, you can create two enabled workspace level delivery configurations
        per workspace for each log type, meaning the same workspace ID can occur in
        the workspace filter for no more than two delivery configurations per log
        type.


        You cannot delete a log delivery configuration, but you can disable it (see
        [Enable or disable log delivery configuration](#operation/patch-log-delivery-config-status)).'
      operationId: create-log-delivery-config
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WrappedCreateLogDeliveryConfiguration'
        description: Properties of the new log delivery configuration.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'
          description: The log delivery configuration creation request succeeded.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create a new log delivery configuration
      tags:
      - Log delivery configurations
  /accounts/{account_id}/log-delivery/{log_delivery_configuration_id}:
    get:
      description: Get a Databricks log delivery configuration object for an account,
        both specified by ID.
      operationId: get-log-delivery-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'
          description: The log delivery configuration was successfully returned.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get log delivery configuration
      tags:
      - Log delivery configurations
    parameters:
    - $ref: '#/components/parameters/account_id'
    - description: Databricks log delivery configuration ID
      in: path
      name: log_delivery_configuration_id
      required: true
      schema:
        format: uuid
        type: string
    patch:
      description: Enable or disable a log delivery configuration. Deletion of delivery
        configurations is not supported, so disable log delivery configurations that
        are no longer needed. Note that you can't re-enable a delivery configuration
        if this would violate the delivery configuration limits described under [Create
        log delivery](#operation/create-log-delivery-config).
      operationId: patch-log-delivery-config-status
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpdateLogDeliveryConfigurationStatusRequest'
        description: The new status for this log delivery configuration object.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/WrappedLogDeliveryConfiguration'
          description: The log delivery configuration was successfully updated.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Enable or disable log delivery configuration
      tags:
      - Log delivery configurations
  /accounts/{account_id}/networks:
    get:
      description: 'Get a list of all Databricks network configurations for an account,
        specified by ID.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-network-configs
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListNetworksResponse'
          description: The network configurations were successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all network configurations
      tags:
      - Network configurations
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: 'Create a Databricks network configuration that represents an AWS
        VPC and its resources. The VPC will be used for new Databricks clusters. This
        requires a pre-created VPC and subnets. For VPC requirements, see [Customer-managed
        VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html).


        **Important**: You can share one customer-managed VPC with multiple workspaces
        in a single account. Therefore, you can share one VPC across multiple Account
        API network configurations. However, you **cannot** reuse subnets or Security
        Groups between workspaces.  Because a Databricks Account API network configuration
        encapsulates this information, you cannot reuse a Databricks Account API network
        configuration across workspaces. If you plan to share one VPC with multiple
        workspaces, be sure to size your VPC and subnets accordingly.

        For detailed instructions of creating a new workspace with this API, see [Create
        a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: create-network-config
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateNetworkRequest'
        description: Properties of the new network configuration.
        required: true
      responses:
        '201':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Network'
          description: The network configuration was successfully created.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create network configuration
      tags:
      - Network configurations
  /accounts/{account_id}/networks/{network_id}:
    delete:
      description: 'Delete a Databricks network configuration, which represents an
        AWS VPC and its resources. You cannot delete a network that is associated
        with a workspace.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: delete-network-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Network'
          description: The network configuration was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete network configuration
      tags:
      - Network configurations
    get:
      description: 'Get a Databricks network configuration, which represents an AWS
        VPC and its resources.  This requires a pre-created VPC and subnets. For VPC
        requirements, see [Customer-managed VPC](http://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html).


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-network-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Network'
          description: The network configuration was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get a network configuration
      tags:
      - Network configurations
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Databricks Account API network configuration ID.
      in: path
      name: network_id
      required: true
      schema:
        format: uuid
        type: string
  /accounts/{account_id}/private-access-settings:
    get:
      description: 'Get a list of all private access settings objects for an account,
        specified by ID.


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: get-private-access-settings-objects
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListPrivateAccessSettingsResponse'
          description: The private access settings object was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all private access settings objects
      tags:
      - 'AWS PrivateLink: private access settings'
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: 'Create a private access settings object, which specifies how your
        workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).
        To use AWS PrivateLink, a workspace must have a private access settings object
        referenced by ID in the workspace''s `private_access_settings_id` property.


        You can share one private access settings with multiple workspaces in a single
        account.However, private access settings are region specific, so only workspaces
        in the same region may use a given private access settings object.


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: create-private-access-settings
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpsertPrivateAccessSettingsRequest'
        description: Properties of the new private access settings object.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PrivateAccessSettings'
          description: The private access settings object was successfully created.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create private access settings
      tags:
      - 'AWS PrivateLink: private access settings'
  /accounts/{account_id}/private-access-settings/{private-access-settings-id}:
    delete:
      description: 'Delete a private access settings object, which determines how
        your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: delete-private-access-settings-object
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PrivateAccessSettings'
          description: The private access settings was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete a private access settings object
      tags:
      - 'AWS PrivateLink: private access settings'
    get:
      description: 'Get a private access settings object, which specifies how your
        workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: get-private-access-settings-object
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PrivateAccessSettings'
          description: The private access settings object was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get a private access settings object
      tags:
      - 'AWS PrivateLink: private access settings'
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Databricks Account API private access settings ID.
      in: path
      name: private-access-settings-id
      required: true
      schema:
        format: uuid
        type: string
    put:
      description: 'Update an existing private access settings object, which specifies
        how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).
        To use AWS PrivateLink, a workspace must have a private access settings object
        referenced by ID in the workspace''s `private_access_settings_id` property.


        This operation fully overwrites your existing private access settings object
        attached to your workspaces. All workspaces attached to the private access
        settings will see the effects of any change. If updating `public_access_enabled`,
        `private_access_level`, or `allowed_vpc_endpoint_ids`, effects of the change
        may take a couple minutes to propagate to the workspace API.

        You can share one private access settings with multiple workspaces in a single
        account. However, private access settings are region specific, so only workspaces
        in the same region may use a given private access settings object.


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: replace-private-access-settings
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpsertPrivateAccessSettingsRequest'
        description: Properties of the new private access settings object.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/PrivateAccessSettings'
          description: The private access settings object was successfully updated.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Replace private access settings
      tags:
      - 'AWS PrivateLink: private access settings'
  /accounts/{account_id}/storage-configurations:
    get:
      description: Get a list of all Databricks storage configurations for your account,
        specified by ID.
      operationId: get-storage-configs
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListStorageConfigurationsResponse'
          description: The storage configurations were successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all storage configurations
      tags:
      - Storage configurations
    parameters:
    - $ref: '#/components/parameters/account_id_dual_use'
    post:
      description: 'Create new storage configuration for an account, specified by
        ID. Uploads a storage configuration object that represents the root AWS S3
        bucket in your account. Databricks stores related workspace assets including
        DBFS, cluster logs, and job results. For AWS S3 bucket, you need to configure
        the required bucket policy.


        For detailed instructions of creating a new workspace with this API, see [Create
        a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)'
      operationId: create-storage-config
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateStorageConfigurationRequest'
        description: Properties of the new storage configuration.
        required: true
      responses:
        '201':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/StorageConfiguration'
          description: The storage configuration was successfully created.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create new storage configuration
      tags:
      - Storage configurations
  /accounts/{account_id}/storage-configurations/{storage_configuration_id}:
    delete:
      description: Delete a Databricks storage configuration. You cannot delete a
        storage configuration that is currently being associated to any workspace.
      operationId: delete-storage-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/StorageConfiguration'
          description: The storage configuration was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete storage configuration
      tags:
      - Storage configurations
    get:
      description: Get a Databricks storage configuration for an account, both specified
        by ID.
      operationId: get-storage-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/StorageConfiguration'
          description: The storage configuration was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get storage configuration
      tags:
      - Storage configurations
    parameters:
    - $ref: '#/components/parameters/account_id_dual_use'
    - description: Databricks Account API storage configuration ID.
      in: path
      name: storage_configuration_id
      required: true
      schema:
        format: uuid
        type: string
  /accounts/{account_id}/usage/download:
    get:
      description: Return billable usage logs in CSV format for the specified account
        and date range. See [CSV file schema](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html#csv-file-schema)
        for the data schema. Note that this method may take multiple seconds to complete.
      operationId: download-billable-usage
      parameters:
      - description: 'Format: `YYYY-MM`. First month to return billable usage logs
          for. This field is required.'
        in: query
        name: start_month
        required: true
        schema:
          $ref: '#/components/schemas/UsageDownloadMonth'
      - description: 'Format: `YYYY-MM`. Last month to return billable usage logs
          for. This field is required.'
        in: query
        name: end_month
        required: true
        schema:
          $ref: '#/components/schemas/UsageDownloadMonth'
      - default: false
        description: Specify whether to include personally identifiable information
          in the billable usage logs, for example the email addresses of cluster creators.
          Handle this information with care. Defaults to false.
        in: query
        name: personal_data
        schema:
          type: boolean
      responses:
        '200':
          content:
            application/csv:
              schema:
                type: string
          description: Billable usage data was returned successfully.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Return billable usage logs in CSV format for the specified account
        and date range.
      tags:
      - Billable usage download
    parameters:
    - $ref: '#/components/parameters/account_id'
  /accounts/{account_id}/vpc-endpoints:
    get:
      description: 'Get a list of all VPC endpoints for an account, specified by ID.


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: get-vpc-endpoints
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListVPCEndpointsResponse'
          description: The VPC endpoints were successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all VPC endpoint configurations
      tags:
      - 'AWS PrivateLink: VPC endpoint configurations'
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: 'Create a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html)
        object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).


        **IMPORTANT**: When you register a VPC endpoint to the Databricks workspace
        VPC endpoint service for any workspace, **in this release <Databricks> enables
        front-end (web application and REST API) access from the source network of
        the VPC endpoint to all workspaces in that AWS region in your <Databricks>
        account if the workspaces have any PrivateLink connections in their workspace
        configuration**. If you have questions about this behavior, contact your Databricks
        representative.


        Within AWS, your VPC endpoint stays in `pendingAcceptance` state until you
        register it in a VPC endpoint configuration through the Account API. Upon
        doing so, the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html)
        automatically accepts the VPC endpoint and it eventually transitions to the
        `available` state.


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: create-vpc-endpoint
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateVPCEndpointRequest'
        description: Properties of the new VPC endpoint configuration.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VPCEndpoint'
          description: The VPC endpoint configuration was successfully created.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create VPC endpoint configuration
      tags:
      - 'AWS PrivateLink: VPC endpoint configurations'
  /accounts/{account_id}/vpc-endpoints/{vpc-endpoint-id}:
    delete:
      description: 'Delete a VPC endpoint configuration, which represents an [AWS
        VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html)
        that can communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).


        Upon deleting a VPC endpoint configuration, the VPC endpoint in AWS changes
        its state from `accepted` to `rejected`, meaning it will no longer be usable
        from your VPC.


        Before configuring PrivateLink, it is important to read the [Databricks article
        about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: delete-vpc-endpoint
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VPCEndpoint'
          description: The VPC endpoint configuration was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete VPC endpoint configuration
      tags:
      - 'AWS PrivateLink: VPC endpoint configurations'
    get:
      description: 'Get a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html)
        object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).


        This operation is available only if your account is on the E2 version of the
        platform and your Databricks account is enabled for PrivateLink (Public Preview).
        Contact your Databricks representative to enable your account for PrivateLink.'
      operationId: get-vpc-endpoint-config
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VPCEndpoint'
          description: The VPC endpoint was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get a VPC endpoint configuration
      tags:
      - 'AWS PrivateLink: VPC endpoint configurations'
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Databricks VPC endpoint ID.
      in: path
      name: vpc-endpoint-id
      required: true
      schema:
        format: uuid
        type: string
  /accounts/{account_id}/workspaces:
    get:
      description: 'Get a list of all workspaces associated with an account, specified
        by ID.


        This operation is available only if your account is on the E2 version of the
        platform or on a select custom plan that allows multiple workspaces per account.'
      operationId: get-workspaces
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListWorkspacesResponse'
          description: The workspaces were returned successfully.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get all workspaces
      tags:
      - Workspaces
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    post:
      description: 'Create a new workspace using a credential configuration and a
        storage configuration, an optional network configuration (if using a customer-managed
        VPC), an optional managed services key configuration (if using customer-managed
        keys for managed services), and an optional storage key configuration (if
        using customer-managed keys for storage). The key configurations used for
        managed services and storage encryption may be the same or different.


        **Important**: This operation is asynchronous. A response with HTTP status
        code 200 means the request has been accepted and is in progress, but does
        not mean that the workspace deployed successfully and is running. The initial
        workspace status is typically  `PROVISIONING`. Use the workspace ID (`workspace_id`)
        field in the response to identify the new workspace and make repeated `GET`
        requests with the workspace ID and check its status. The workspace becomes
        available when the status changes to `RUNNING`.


        You can share one customer-managed VPC with multiple workspaces in a single
        account. It is not required to create a new VPC for each workspace. However,
        you **cannot** reuse subnets or Security Groups between workspaces. If you
        plan to share one VPC with multiple workspaces, be sure to size your VPC and
        subnets accordingly. Because a Databricks Account API network configuration
        encapsulates this information, you cannot reuse a Databricks Account API network
        configuration across workspaces.

        For detailed instructions of creating a new workspace with this API **including
        error handling** see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).


        **Important**: Customer-managed VPCs, PrivateLink, and customer-managed keys
        are supported on a limited set of deployment and subscription types. If you
        have questions about availability, contact your Databricks representative.


        This operation is available only if your account is on the E2 version of the
        platform or on a select custom plan that allows multiple workspaces per account.'
      operationId: create-workspace
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateWorkspaceRequest'
        description: Properties of the new workspace.
        required: true
      responses:
        '201':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Workspace'
          description: Workspace creation request was received. Check workspace status.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '403':
          $ref: '#/components/responses/Forbidden'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Create a new workspace
      tags:
      - Workspaces
  /accounts/{account_id}/workspaces/{workspace_id}:
    delete:
      description: 'Terminate and delete a Databricks workspace. From an API perspective,
        deletion is immediate. However, it may take a few minutes for all workspaces
        resources to be deleted, depending on the size and number of workspace resources.


        This operation is available only if your account is on the E2 version of the
        platform or on a select custom plan that allows multiple workspaces per account.'
      operationId: delete-workspace
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Workspace'
          description: The workspace was successfully deleted.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Delete workspace
      tags:
      - Workspaces
    get:
      description: 'Get information including status for a Databricks workspace, specified
        by ID. In the response, the `workspace_status` field indicates the current
        status. After initial workspace creation (which is asynchronous), make repeated
        `GET` requests with the workspace ID and check its status. The workspace becomes
        available when the status changes to `RUNNING`.


        For detailed instructions of creating a new workspace with this API **including
        error handling** see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).


        This operation is available only if your account is on the E2 version of the
        platform or on a select custom plan that allows multiple workspaces per account.'
      operationId: get-workspace
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Workspace'
          description: The workspace configuration was successfully returned.
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '404':
          $ref: '#/components/responses/NotFound'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get workspace
      tags:
      - Workspaces
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Workspace ID.
      in: path
      name: workspace_id
      required: true
      schema:
        format: int64
        type: integer
    patch:
      description: "The `PATCH` operation on this endpoint can update a workspace\
        \ configuration for either a running workspace or a failed workspace. The\
        \ elements that can be updated varies between these two use cases.\n\n###\
        \ Update a failed workspace\nYou can update a Databricks workspace configuration\
        \ for failed workspace deployment for some but not all fields. This request\
        \ supports updating only the following fields of a failed workspace:\n- Credential\
        \ configuration ID\n- Storage configuration ID\n- Network configuration ID.\
        \ Used only if you use customer-managed VPC.\n- Key configuration ID for managed\
        \ services (control plane storage, such as notebook source and Databricks\
        \ SQL queries). Used only if you use customer-managed keys for managed services.\n\
        - Key configuration ID for workspace storage (root S3 bucket and optionally\
        \ EBS volumes). Used only if you use customer-managed keys for workspace storage.\
        \ IMPORTANT: If the workspace was ever in the running state, even if briefly\
        \ before becoming a failed workspace, you cannot add a new key configuration\
        \ ID for workspace storage.\n\nAfter calling the `PATCH` operation to update\
        \ the workspace configuration, make repeated `GET` requests with the workspace\
        \ ID and check the workspace status. The workspace is successful if the status\
        \ changes to `RUNNING`.\n\nFor detailed instructions of creating a new workspace\
        \ with this API **including error handling** see [Create a new workspace using\
        \ the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n\
        \n### Update a running workspace\nYou can update a Databricks workspace configuration\
        \ for running workspaces for some but not all fields. This request supports\
        \ updating only the following fields of a running workspace:\n- Credential\
        \ configuration ID\n\n- Network configuration ID. Used only if you already\
        \ use use customer-managed VPC. This change is supported only if you specified\
        \ a network configuration ID in your original workspace creation. In other\
        \ words, you cannot switch from a Databricks-managed VPC to a customer-managed\
        \ VPC. Note: You cannot use a network configuration update in this API to\
        \ add support for PrivateLink (in Public Preview). To add PrivateLink to an\
        \ existing workspace, contact your Databricks representative. \n\n- Key configuration\
        \ ID for managed services (control plane storage, such as notebook source\
        \ and Databricks SQL queries). Databricks does not directly encrypt the data\
        \ with the customer-managed key (CMK). Databricks uses both the CMK and the\
        \ Databricks managed key (DMK) that is unique to your workspace to encrypt\
        \ the Data Encryption Key (DEK). Databricks uses the DEK to encrypt your workspace's\
        \ managed services persisted data. If the workspace does not already have\
        \ a CMK for managed services, adding this ID enables managed services encryption\
        \ for new or updated data. Existing managed services data that existed before\
        \ adding the key remains not encrypted with the DEK until modified. If the\
        \ workspace already has customer-managed keys for managed services, this request\
        \ rotates (changes) the CMK keys and the DEK is re-encrypted with the DMK\
        \ and the new CMK.\n- Key configuration ID for workspace storage (root S3\
        \ bucket and optionally EBS volumes). You can set this only if the workspace\
        \ does not already have a customer-managed key configuration for workspace\
        \ storage. \n\n**Important**: For updating running workspaces, this API is\
        \ unavailable on Mondays, Tuesdays, and Thursdays from 4:30pm-7:30pm PST due\
        \ to routine maintenance. Plan your workspace updates accordingly. For questions\
        \ about this schedule, contact your Databricks representative.\n\n**Important**:\
        \ To update a running workspace, your workspace must have no running cluster\
        \ instances, which includes all-purpose clusters, job clusters, and pools\
        \ that may have running clusters. Terminate all cluster instances in the workspace\
        \ before calling this API. \n\n### Wait until changes take effect\nAfter calling\
        \ the `PATCH` operation to update the workspace configuration, make repeated\
        \ `GET` requests with the workspace ID and check the workspace status and\
        \ the status of the fields.\n* For workspaces with a Databricks-managed VPC,\
        \ the workspace status becomes `PROVISIONING` temporarily (typically under\
        \ 20 minutes). If the workspace update is successful, the workspace status\
        \ changes to `RUNNING`. Note that you can also check the workspace status\
        \ in the [Account Console](https://docs.databricks.com/administration-guide/account-settings-e2/account-console-e2.html).\
        \ However, you cannot use or create clusters for another 20 minutes after\
        \ that status change. This results in a total of up to 40 minutes in which\
        \ you cannot create clusters. If you create or use clusters before this time\
        \ interval elapses, clusters do not launch successfully, fail, or could cause\
        \ other unexpected behavior.  \n\n* For workspaces with a customer-managed\
        \ VPC, the workspace status stays at status `RUNNING` and the VPC change happens\
        \ immediately. A change to the storage customer-managed key configuration\
        \ ID may take a few minutes to update, so continue to check the workspace\
        \ until you observe it has updated. If the update fails, the workspace may\
        \ revert silently to its original configuration. Once the workspace has updated,\
        \ you cannot use or create clusters for another 20 minutes. If you create\
        \ or use clusters before this time interval elapses, clusters do not launch\
        \ successfully, fail, or could cause other unexpected behavior.\n\nIf you\
        \ update the _storage_ customer-managed key configurations, it takes 20 minutes\
        \ for the changes to fully take effect. During the 20 minute wait, it is important\
        \ that you stop all REST API calls to the DBFS API. If you are modifying _only\
        \ the managed services key configuration_, you can omit the 20 minute wait.\n\
        \n**Important**: Customer-managed keys and customer-managed VPCs are supported\
        \ by only some deployment types and subscription types. If you have questions\
        \ about availability, contact your Databricks representative.\n\nThis operation\
        \ is available only if your account is on the E2 version of the platform or\
        \ on a select custom plan that allows multiple workspaces per account."
      operationId: patch-workspace
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/UpdateWorkspaceRequest'
        description: Changes of the workspace properties.
        required: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Workspace'
          description: The workspace update request is accepted.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '403':
          $ref: '#/components/responses/Forbidden'
        '404':
          $ref: '#/components/responses/NotFound'
        '409':
          $ref: '#/components/responses/Conflict'
        '500':
          $ref: '#/components/responses/InternalError'
        '509':
          $ref: '#/components/responses/ServiceUnavailable'
      summary: Update workspace configuration
      tags:
      - Workspaces
  /accounts/{account_id}/workspaces/{workspace_id}/customer-managed-key-history:
    get:
      description: 'Given a workspace specified by ID, this request gets a list of
        all associations with key configuration objects that encapsulate customer-managed
        keys that encrypt managed services, workspace storage, or in some cases both.


        **Important**: In the current implementation, keys cannot be rotated or removed
        from a workspace. It is possible for a workspace to show a storage customer-managed
        key having been attached and then detached if the workspace was updated to
        use the key and the update operation failed.


        **Important**: Customer-managed keys are supported only for some deployment
        types and subscription types.


        This operation is available only if your account is on the E2 version of the
        platform.'
      operationId: get-workspace-key-history
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListWorkspaceEncryptionKeyRecordsResponse'
          description: The workspace's key history was successfully returned.
        '400':
          $ref: '#/components/responses/BadRequest'
        '401':
          $ref: '#/components/responses/Unauthenticated'
        '500':
          $ref: '#/components/responses/InternalError'
      summary: Get the history of a workspace's associations with keys
      tags:
      - Workspaces
    parameters:
    - $ref: '#/components/parameters/account_id_e2'
    - description: Workspace ID.
      in: path
      name: workspace_id
      required: true
      schema:
        format: int64
        type: integer
security:
- basicAuth: []
servers:
- description: The Account API is published on `accounts.cloud.databricks.com` for
    all AWS regional deployments.
  url: https://accounts.cloud.databricks.com/api/2.0
tags:
- description: These APIs manage credential configurations for this workspace. Databricks
    needs access to a cross-account service IAM role in your AWS account so that Databricks
    can deploy clusters in the appropriate VPC for the new workspace. A credential
    configuration encapsulates this role information, and its ID is used when creating
    a new workspace.
  name: Credential configurations
- description: 'These APIs manage storage configurations for this workspace. A root
    storage S3 bucket in your account is required to store objects like cluster logs,
    notebook revisions, and job results. You can also use the root storage S3 bucket
    for storage of non-production DBFS data. A storage configuration encapsulates
    this bucket information, and its ID is used when creating a new workspace. '
  name: Storage configurations
- description: These APIs manage network configurations for customer-managed VPCs
    (optional). A network configuration encapsulates the IDs for AWS VPCs, subnets,
    and security groups. Its ID is used when creating a new workspace if you use customer-managed
    VPCs.
  name: Network configurations
- description: 'These APIs manage encryption key configurations for this workspace
    (optional). A key configuration encapsulates the AWS KMS key information and some
    information about how the key configuration can be used. There are two possible
    uses for key configurations:


    * Managed services: A key configuration can be used to encrypt a workspace''s
    notebook and secret data in the control plane, as well as Databricks SQL queries
    and query history.


    * Storage: A key configuration can be used to encrypt a workspace''s DBFS and
    EBS data in the data plane.


    In both of these cases, the key configuration''s ID is used when creating a new
    workspace.


    This Preview feature is available if your account is on the E2 version of the
    platform. Updating a running workspace with workspace storage encryption requires
    that the workspace is on the E2 version of the platform. If you have an older
    workspace, it might not be on the E2 version of the platform. If you are not sure,
    contact your Databricks reprsentative.'
  name: Key configurations
- description: 'These APIs manage workspaces for this account. A Databricks workspace
    is an environment for accessing all of your Databricks assets. The workspace organizes
    objects (notebooks, libraries, and experiments) into folders, and provides access
    to data and computational resources such as clusters and jobs.


    These endpoints are available if your account is on the E2 version of the platform
    or on a select custom plan that allows multiple workspaces per account.'
  name: Workspaces
- description: "These APIs manage log delivery configurations for this account. The\
    \ two supported log types for this API are _billable usage logs_ and _audit logs_.\
    \ This feature is in Public Preview. This feature works with all account ID types.\n\
    \nLog delivery works with all account types. However, if your account is on the\
    \ E2 version of the platform or on a select custom plan that allows multiple workspaces\
    \ per account, you can optionally configure different storage destinations for\
    \ each workspace.\n\nLog delivery status is also provided to know the latest status\
    \ of log delivery attempts.\n\nThe high-level flow of billable usage delivery:\n\
    \n1. **Create storage**: In AWS, [create a new AWS S3 bucket](https://docs.databricks.com/administration-guide/account-api/aws-storage.html)\
    \ with a specific bucket policy. Using Databricks APIs, call the Account API to\
    \ create a [storage configuration object](#operation/create-storage-config) that\
    \ uses the bucket name.\n\n2. **Create credentials**: In AWS, create the appropriate\
    \ AWS IAM role. For full details, including the required IAM role policies and\
    \ trust relationship, see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\
    \ Using Databricks APIs, call the Account API to create a [credential configuration\
    \ object](#operation/create-credential-config) that uses the IAM role's ARN. \n\
    \n3. **Create log delivery configuration**: Using Databricks APIs, call the Account\
    \ API to [create a log delivery configuration](#operation/create-log-delivery-config)\
    \ that uses the credential and storage configuration objects from previous steps.\
    \ You can specify if the logs should include all events of that log type in your\
    \ account (_Account level_ delivery) or only events for a specific set of workspaces\
    \ (_workspace level_ delivery). Account level log delivery applies to all current\
    \ and future workspaces plus account level logs, while workspace level log delivery\
    \ solely delivers logs related to the specified workspaces. You can create multiple\
    \ types of delivery configurations per account.\n\nFor billable usage delivery:\n\
    \n* For more information about billable usage logs, see [Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\
    \ For the CSV schema, see the [Usage page](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n\
    * The delivery location is `<bucket-name>/<prefix>/billable-usage/csv/`, where\
    \ `<prefix>` is the name of the optional delivery path prefix you set up during\
    \ log delivery configuration. Files are named `workspaceId=<workspace-id>-usageMonth=<month>.csv`.\n\
    * All billable usage logs apply to specific workspaces (_workspace level_ logs).\
    \ You can aggregate usage for your entire account by creating an _account level_\
    \ delivery configuration that delivers logs for all current and future workspaces\
    \ in your account.\n* The files are delivered daily by overwriting the month's\
    \ CSV file for each workspace.\n\nFor audit log delivery:\n\n* For more information\
    \ about about audit log delivery, see [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html),\
    \ which includes information about the used JSON schema.\n* The delivery location\
    \ is `<bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs_<internal-id>.json`.\
    \ Files may get overwritten with the same content multiple times to achieve exactly-once\
    \ delivery.\n* If the audit log delivery configuration included specific workspace\
    \ IDs, only _workspace-level_ audit logs for those workspaces are delivered. If\
    \ the log delivery configuration applies to the entire account (_account level_\
    \ delivery configuration), the audit log delivery includes workspace-level audit\
    \ logs for all workspaces in the account as well as account-level audit logs.\
    \ See [Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)\
    \ for details.\n* Auditable events are typically available in logs within 15 minutes."
  name: Log delivery configurations
- description: 'This API allows you to download billable usage logs for the specified
    account and date range.


    This feature works with all account types.'
  name: Billable usage download
- description: '**Budgets feature is in [Private Preview](http://docs.databricks.com/release-notes/release-types.html).**


    These APIs manage budget configuration including notifications for exceeding a
    budget for a period. They can also retrieve the status of each budget. '
  name: Budgets
- description: '**PrivateLink is in [Public Preview](http://docs.databricks.com/release-notes/release-types.html).**


    These APIs manage private access settings for this account. A private access settings
    object specifies how your workspace is accessed using AWS PrivateLink. Each workspace
    that has any PrivateLink connections must include the ID for a private access
    settings object is in its workspace configuration object. Your account must be
    enabled for PrivateLink to use these APIs. Before configuring PrivateLink, it
    is important to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).'
  name: 'AWS PrivateLink: private access settings'
- description: '**PrivateLink is in [Public Preview](http://docs.databricks.com/release-notes/release-types.html).**


    These APIs manage VPC endpoint configurations for this account. This object registers
    an AWS VPC endpoint in your Databricks account so your workspace can use it with
    AWS PrivateLink. Your VPC endpoint connects to one of two VPC endpoint services
    -- one for workspace (both for front-end connection and for back-end connection
    to REST APIs) and one for the back-end secure cluster connectivity relay from
    the data plane. Your account must be enabled for PrivateLink to use these APIs.
    Before configuring PrivateLink, it is important to read the [Databricks article
    about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).'
  name: 'AWS PrivateLink: VPC endpoint configurations'
